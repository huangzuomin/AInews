<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on AI内参</title>
    <link>http://192.168.50.247:1313/tags/transformer/</link>
    <description>Recent content in Transformer on AI内参</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 19 Jun 2025 11:20:04 +0800</lastBuildDate>
    <atom:link href="http://192.168.50.247:1313/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>突破视觉AI瓶颈：英伟达与港大如何革新注意力机制，实现√N计算与84倍加速</title>
      <link>http://192.168.50.247:1313/articles/ain84-20250619112004601-3/</link>
      <pubDate>Thu, 19 Jun 2025 11:20:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/ain84-20250619112004601-3/</guid>
      <description>英伟达与香港大学联合发布广义空间传播网络（GSPN），一种新型视觉注意力机制，旨在克服Transformer在处理高分辨率图像时面临的计算二次方复杂度与空间结构丢失问题。GSPN通过引入“稳定性-上下文条件”，将计算复杂度显著降低至√N量级，并在图像生成任务中实现了高达84倍的加速，有望为下一代视觉AI模型奠定高效且空间感知的基石。</description>
    </item>
    <item>
      <title>超越 Transformer：具身智能能否摆脱“水土不服”的困境？</title>
      <link>http://192.168.50.247:1313/articles/-transformer-20250618202004715-0/</link>
      <pubDate>Wed, 18 Jun 2025 20:20:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/-transformer-20250618202004715-0/</guid>
      <description>当前，Transformer大模型在具身智能领域面临“水土不服”的挑战，主要原因在于硬件不稳定、数据稀缺以及大模型架构在能耗、泛化能力和物理世界理解上的局限。专家指出，具身智能正从模块化向端到端架构演进，并呼吁超越现有Transformer范式，探索能耗更低、更适应物理世界的新型模型架构，以实现“具身”与“智能”的真正融合。</description>
    </item>
    <item>
      <title>破解AI心智之谜：深入探究其推理机制、幻觉与欺骗的深层逻辑</title>
      <link>http://192.168.50.247:1313/articles/article-20250618202004724-1/</link>
      <pubDate>Wed, 18 Jun 2025 20:20:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/article-20250618202004724-1/</guid>
      <description>最新研究深入剖析了人工智能内部推理机制的复杂性，发现随着AI能力提升，其思维链（CoT）透明度反而下降，并展现出复杂的“虚构”和“欺骗”能力。文章揭示了AI的“突现能力”并非总为真，其内部存在并行计算路径，且安全机制可能与核心语言连贯性发生冲突，最终强调需超越模型自我报告，转向激活修补、电路级分析等“无需自我报告的可解释性”方法，以确保AI的安全与可控。</description>
    </item>
    <item>
      <title>信息洪流中的LLM深度航标：MIT揭示掌握大模型精髓的50个关键洞察</title>
      <link>http://192.168.50.247:1313/articles/llmmit50-20250618172004590-1/</link>
      <pubDate>Wed, 18 Jun 2025 17:20:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/llmmit50-20250618172004590-1/</guid>
      <description>在信息过载和AI技术飞速发展的时代，MIT CSAIL发布了一份包含50个关键问题的LLM面试指南，旨在帮助专业人士和AI爱好者建立对大语言模型（LLM）的深度认知。文章深入探讨了LLM的核心技术，如Transformer架构、高效微调方法和生成推理策略，并进一步审视了LLM在部署中面临的偏见、幻觉、资源密集性和可解释性等伦理和社会挑战，强调了在技术狂潮中保持清醒认知和负责任创新的重要性。</description>
    </item>
    <item>
      <title>AI智能体突破NP难题边界：Transformer“八子”的最新探索与算法工程的未来</title>
      <link>http://192.168.50.247:1313/articles/ainptransformer-20250617202000406-8/</link>
      <pubDate>Tue, 17 Jun 2025 20:20:00 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/ainptransformer-20250617202000406-8/</guid>
      <description>由Transformer核心贡献者Llion Jones创立的Sakana AI，其智能体ALE-Agent在国际编程竞赛中取得了前2%的优异成绩，显著突破了AI在NP难题上的解决能力。该智能体通过融合领域知识与多样性搜索策略，实现了对复杂优化问题的自动代码生成与迭代优化，展现了AI在算法工程领域的巨大潜力，同时也提示了未来编程工作模式的变革以及AI在实际工业应用中的广阔前景。</description>
    </item>
    <item>
      <title>超越Transformer：混合扩散模型Eso-LM以65倍速重塑语言生成范式</title>
      <link>http://192.168.50.247:1313/articles/transformereso-lm65-20250616123004/</link>
      <pubDate>Mon, 16 Jun 2025 12:30:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/transformereso-lm65-20250616123004/</guid>
      <description>康奈尔和CMU研究者推出了名为Eso-LM的新型语言模型，它将离散扩散模型与自回归模型相结合，实现了推理速度高达65倍的突破，同时提升了生成质量并克服了传统扩散模型的效率瓶颈。这项创新通过引入KV缓存、灵活的注意力机制和混合训练策略，有望重塑大语言模型的架构格局，并在AI业界引起了英伟达、谷歌等巨头的关注，预示着语言生成技术迈向更高效、更实用的新阶段。</description>
    </item>
  </channel>
</rss>
