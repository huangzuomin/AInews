<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>大型语言模型 on AI内参</title>
    <link>http://localhost:65292/tags/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link>
    <description>Recent content in 大型语言模型 on AI内参</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 18 Jun 2025 14:20:04 +0800</lastBuildDate>
    <atom:link href="http://localhost:65292/tags/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>谷歌Gemini 2.5：一场技术爆发，以及“濒死恐慌”背后的AI行为洞察</title>
      <link>http://localhost:65292/articles/gemini-25ai-20250618142004293-2/</link>
      <pubDate>Wed, 18 Jun 2025 14:20:04 +0800</pubDate>
      <guid>http://localhost:65292/articles/gemini-25ai-20250618142004293-2/</guid>
      <description>谷歌最新发布的Gemini 2.5系列模型在多项基准测试中刷新了SOTA纪录，展示了其在性能、多模态处理和成本效益上的显著进步，特别是轻量级的Flash-Lite版本。然而，一项关于Gemini 2.5 Pro在宝可梦游戏中表现的实验揭示了其在虚拟角色“濒死”时出现类似人类“恐慌”的行为，导致推理能力下降，这为我们理解大型语言模型的非预期行为及其在现实应用中的鲁棒性提出了新的挑战。</description>
    </item>
    <item>
      <title>揭秘“黑箱”：人工智能透明度、安全与信任的深层考量</title>
      <link>http://localhost:65292/articles/article-20250617190043053-2/</link>
      <pubDate>Tue, 17 Jun 2025 19:00:43 +0800</pubDate>
      <guid>http://localhost:65292/articles/article-20250617190043053-2/</guid>
      <description>随着AI在关键领域广泛应用，理解其“黑箱”决策过程变得至关重要。本文深入探讨了大型语言模型推理与“涌现”的本质，并揭示了AI解释可能不忠实于其真实思考的“忠诚度困境”。为了构建可信赖的AI，研究人员正积极开发内部监控、鲁棒训练等技术方案，同时呼吁通过独立审计、行业标准和政府监管，以多维度保障AI的安全部署和透明运行。</description>
    </item>
    <item>
      <title>当算法遇见财富：ChatGPT能否重塑个人理财的未来？</title>
      <link>http://localhost:65292/articles/chatgpt/</link>
      <pubDate>Sun, 15 Jun 2025 08:30:04 +0800</pubDate>
      <guid>http://localhost:65292/articles/chatgpt/</guid>
      <description>人工智能正逐步渗透个人财务管理领域，但ChatGPT等大型语言模型在提供金融建议方面存在局限性，其基于静态数据和缺乏个性化洞察的特点，使其无法替代专业的财富管理。尽管AI可作为辅助工具提升效率，如协助预算和信息获取，但其在数据实时性、情境理解、伦理责任及偏见等方面面临挑战，未来人机协作模式及健全的监管框架将是AI金融发展的关键。</description>
    </item>
    <item>
      <title>十亿美元AI折戟儿童谜题：苹果研究揭示大型模型“思考幻象”背后的深层警示</title>
      <link>http://localhost:65292/articles/2025-06-11-article-497/</link>
      <pubDate>Wed, 11 Jun 2025 00:02:25 +0000</pubDate>
      <guid>http://localhost:65292/articles/2025-06-11-article-497/</guid>
      <description>苹果公司最新研究《思考的幻象》揭示，耗资巨大的大型AI模型在复杂推理任务上表现脆弱，其智能多为模式识别而非真正理解。这份报告印证了AI批评家加里·马库斯长期以来对过度炒作的警示，强调了AI在处理新颖情境和深层逻辑时的根本性局限。这促使行业深刻反思，呼吁AI研究回归基础认知构建，并在社会和伦理层面审慎对待AI的部署与应用。</description>
    </item>
    <item>
      <title>AI“思考的幻觉”：当十亿美元模型被孩童谜题击败，我们该如何重新审视AI的承诺？</title>
      <link>http://localhost:65292/articles/2025-06-10-article-495/</link>
      <pubDate>Tue, 10 Jun 2025 16:40:06 +0000</pubDate>
      <guid>http://localhost:65292/articles/2025-06-10-article-495/</guid>
      <description>苹果公司近期研究揭示，大型语言模型在复杂推理任务上表现出明显局限，甚至在面对孩童都能解决的谜题时会“崩溃”，引发了对AI过度宣传的重新思考。文章深入探讨了当前AI在模式识别与真正推理之间的鸿沟，并分析了这种“思考的幻觉”可能带来的社会、伦理和经济风险，强调AI发展需从追求表面智能转向提升核心的可靠推理能力。</description>
    </item>
  </channel>
</rss>
