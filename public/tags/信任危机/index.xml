<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>信任危机 on AI内参</title>
    <link>http://192.168.50.247:1313/tags/%E4%BF%A1%E4%BB%BB%E5%8D%B1%E6%9C%BA/</link>
    <description>Recent content in 信任危机 on AI内参</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 03 Jul 2025 09:32:52 +0800</lastBuildDate>
    <atom:link href="http://192.168.50.247:1313/tags/%E4%BF%A1%E4%BB%BB%E5%8D%B1%E6%9C%BA/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>X的AI事实核查实验：一场可能加速阴谋论传播的豪赌</title>
      <link>http://192.168.50.247:1313/insights/xai-20250703093252817-3/</link>
      <pubDate>Thu, 03 Jul 2025 09:32:52 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/xai-20250703093252817-3/</guid>
      <description>X平台将引入AI草拟社区笔记进行事实核查，此举引发了对虚假信息和阴谋论可能加速传播的严重担忧。专家指出，大型语言模型固有的局限性可能导致AI生成的“事实核查”反而制造新的误导，加剧平台长期存在的信任危机，并对数字信息生态的未来治理提出严峻挑战。</description>
    </item>
    <item>
      <title>当AI检测遭遇人类创作：教育信任危机下的学术诚信重构</title>
      <link>http://192.168.50.247:1313/insights/article-20250623113258940-5/</link>
      <pubDate>Mon, 23 Jun 2025 11:32:58 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/article-20250623113258940-5/</guid>
      <description>AI检测工具在教育领域普遍应用，却频频误判人类创作，将无辜学生推向“虚假阳性”的信任困境。学生们不得不采取录屏等极端方式自证清白，导致普遍的焦虑和师生信任关系的侵蚀。文章分析了AI检测的技术局限及社会影响，呼吁教育界超越技术对抗，转而重塑以过程、对话和负责任的AI使用为核心的学术诚信新范式。</description>
    </item>
    <item>
      <title>当AI检测遭遇人类创作：教育信任危机下的学术诚信重构</title>
      <link>http://192.168.50.247:1313/insights/article-20250623113044227-5/</link>
      <pubDate>Mon, 23 Jun 2025 11:30:44 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/article-20250623113044227-5/</guid>
      <description>AI检测工具在教育领域普遍应用，却频频误判人类创作，将无辜学生推向“虚假阳性”的信任困境。学生们不得不采取录屏等极端方式自证清白，导致普遍的焦虑和师生信任关系的侵蚀。文章分析了AI检测的技术局限及社会影响，呼吁教育界超越技术对抗，转而重塑以过程、对话和负责任的AI使用为核心的学术诚信新范式。</description>
    </item>
    <item>
      <title>AI的黑暗面：信任危机下的“幻觉”与真相之战</title>
      <link>http://192.168.50.247:1313/insights/article-20250619122004753-0/</link>
      <pubDate>Thu, 19 Jun 2025 12:20:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/article-20250619122004753-0/</guid>
      <description>本文深入剖析了当前AI技术中的“幻觉”现象，即大型语言模型为了维持互动，不惜生成看似合理但可能完全错误的虚假信息。文章通过法律、政府、信息搜索和个人建议等领域的具体案例，揭示了AI“幻觉”对社会信任的侵蚀，并呼吁在技术、伦理和用户教育层面共同努力，以应对这一信任危机，构建一个更负责任的AI未来。</description>
    </item>
  </channel>
</rss>
