<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MiniMax on AI内参</title>
    <link>http://localhost:1313/tags/minimax/</link>
    <description>Recent content in MiniMax on AI内参</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 18 Jun 2025 12:20:04 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/minimax/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>中国大模型“六小龙”淘汰赛：AI淘金热退潮下的精英流失与战略求生</title>
      <link>http://localhost:1313/articles/article-20250618122004582-2/</link>
      <pubDate>Wed, 18 Jun 2025 12:20:04 +0800</pubDate>
      <guid>http://localhost:1313/articles/article-20250618122004582-2/</guid>
      <description>曾备受瞩目的中国大模型“六小龙”正经历一场严峻的淘汰赛，其核心特征是自2024年以来22位高管的密集离职和公司战略的重大转向。面对DeepSeek等开源模型和互联网巨头的竞争压力，这些初创公司已普遍放弃耗资巨大的基础大模型研发，转而聚焦在AI医疗、产业大模型平台等垂直应用领域，力求在资金紧张、人才流失的困境中，找到实现自我造血的生存之道。</description>
    </item>
    <item>
      <title>MiniMax M1的非共识之路：中国大模型公司如何重塑AI推理的边界</title>
      <link>http://localhost:1313/articles/minimax-m1ai-20250618082004316-0/</link>
      <pubDate>Wed, 18 Jun 2025 08:20:04 +0800</pubDate>
      <guid>http://localhost:1313/articles/minimax-m1ai-20250618082004316-0/</guid>
      <description>MiniMax近日发布了其自研的MiniMax-M1推理模型，这款模型创新性地融合了MoE架构和混合注意力机制，并引入了新型强化学习算法CISPO，显著提升了长上下文理解和智能体工具使用能力，同时大幅降低了训练成本。M1的推出不仅展现了MiniMax在基础模型技术上的深厚实力，也再次强调了其作为一家“模型驱动”AI公司的核心战略定位。</description>
    </item>
    <item>
      <title>MiniMax M1：解构中国AI“六小虎”的首个开源推理模型，重塑长上下文交互的边界</title>
      <link>http://localhost:1313/articles/minimax-m1ai-20250617202000424-10/</link>
      <pubDate>Tue, 17 Jun 2025 20:20:00 +0800</pubDate>
      <guid>http://localhost:1313/articles/minimax-m1ai-20250617202000424-10/</guid>
      <description>MiniMax开源了其首个大规模混合架构推理模型M1，以4560亿参数、MoE架构和独特的“闪电注意力”机制，在长上下文处理和Agent工具使用方面展现出卓越性能，并大幅降低了训练成本。M1的开放标志着中国AI公司在高效、超长上下文推理技术上的重要突破，预示着未来AI在复杂任务协作中的广阔应用前景。</description>
    </item>
  </channel>
</rss>
