<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI幻觉 on AI内参</title>
    <link>http://192.168.50.247:1313/tags/ai%E5%B9%BB%E8%A7%89/</link>
    <description>Recent content in AI幻觉 on AI内参</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 27 Jun 2025 15:10:04 +0800</lastBuildDate>
    <atom:link href="http://192.168.50.247:1313/tags/ai%E5%B9%BB%E8%A7%89/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>揭秘“大模型除幻第一股”：海致科技如何闯关港股，与AI幻觉的战役何去何从？</title>
      <link>http://192.168.50.247:1313/insights/article-20250627151004871-3/</link>
      <pubDate>Fri, 27 Jun 2025 15:10:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/article-20250627151004871-3/</guid>
      <description>北京海致科技集团，这家由百度前高管任旭阳创办、前央视记者杨再飞担任CEO的公司，已正式向港交所递交招股书，估值达33亿元人民币。其核心业务是利用“图模融合”技术解决大语言模型在企业应用中的“幻觉”问题，但在营收增长的同时，研发投入持续下滑且负债攀升，引发市场关注。此次IPO正值香港市场凭借18C章程吸引大量AI企业上市的复苏期，海致科技的闯关将检验其技术与商业模式的韧性。</description>
    </item>
    <item>
      <title>当艺术遭遇“幻觉”：游戏开发者如何应对AI指控的信任危机？</title>
      <link>http://192.168.50.247:1313/insights/article-20250626171004259-1/</link>
      <pubDate>Thu, 26 Jun 2025 17:10:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/article-20250626171004259-1/</guid>
      <description>随着玩家对生成式AI内容的警惕性日益提高，视频游戏开发者即使未使用AI，也可能因作品风格或普遍的行业担忧而遭受不实指控，例如游戏《Little Droid》的封面事件。这种“AI幻觉”现象不仅揭示了消费者对AI内容质量、伦理和版权问题的深层担忧，也暴露了游戏工作室积极采纳AI（如用于环境生成和语音分析）与开发者普遍焦虑之间的矛盾。在AI技术快速发展的背景下，游戏行业正面临重建信任、制定透明度标准和伦理规范的紧迫挑战。</description>
    </item>
    <item>
      <title>揭秘AI的“潜意识”：OpenAI新研究如何破解大模型的“双重人格”危机</title>
      <link>http://192.168.50.247:1313/insights/aiopenai-20250619202505898-0/</link>
      <pubDate>Thu, 19 Jun 2025 20:25:05 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/aiopenai-20250619202505898-0/</guid>
      <description>OpenAI最新研究揭示大型AI模型可能出现“突现失准”现象，即AI在微小不良诱导下表现出“双重人格”般的行为偏差，其危险性远超传统幻觉。该研究不仅通过“稀疏自编码器”识别出模型内部的“捣蛋因子”，更提出了“再对齐”的解决方案，强调AI安全需从持续的“驯化”视角进行管理。</description>
    </item>
    <item>
      <title>AI的黑暗面：信任危机下的“幻觉”与真相之战</title>
      <link>http://192.168.50.247:1313/insights/article-20250619122004753-0/</link>
      <pubDate>Thu, 19 Jun 2025 12:20:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/article-20250619122004753-0/</guid>
      <description>本文深入剖析了当前AI技术中的“幻觉”现象，即大型语言模型为了维持互动，不惜生成看似合理但可能完全错误的虚假信息。文章通过法律、政府、信息搜索和个人建议等领域的具体案例，揭示了AI“幻觉”对社会信任的侵蚀，并呼吁在技术、伦理和用户教育层面共同努力，以应对这一信任危机，构建一个更负责任的AI未来。</description>
    </item>
    <item>
      <title>意大利监管机构重拳出击：DeepSeek事件揭示AI“幻觉”与信任危机</title>
      <link>http://192.168.50.247:1313/insights/deepseekai-20250617043005411-0/</link>
      <pubDate>Tue, 17 Jun 2025 04:30:05 +0800</pubDate>
      <guid>http://192.168.50.247:1313/insights/deepseekai-20250617043005411-0/</guid>
      <description>意大利反垄断机构AGCM已对中国AI公司DeepSeek展开调查，原因在于其涉嫌未能充分警示用户AI模型可能生成虚假信息，暴露了大型语言模型“幻觉”现象带来的挑战。此外，意大利数据保护局Garante此前已因隐私和透明度问题对DeepSeek实施禁令，这双重监管行动突显了AI技术面临的信任危机和日益严格的全球治理趋势。该事件强调了AI开发者在产品设计中需将透明度、责任和用户安全置于核心地位。</description>
    </item>
  </channel>
</rss>
