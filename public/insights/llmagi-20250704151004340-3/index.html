<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>超越符号：杨立昆新研究揭示LLM认知鸿沟，预示AGI之路范式巨变 | AI内参</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="杨立昆的最新研究量化揭示了LLM与人类认知策略的根本差异：LLM擅长统计压缩，而人类侧重适应性理解，预示着单纯扩大模型规模无法实现通用人工智能。文章深入探讨了强化学习、大型概念模型和世界模型等多元化新路径，指出AI发展将从单一的预训练范式转向多模态、物理世界锚定与架构创新相结合，以期弥合认知鸿沟，迈向更具理解力的通用智能。">
    <meta name="generator" content="Hugo 0.147.8">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    <meta name="author" content="温故智新AIGC实验室">
    

    


<link rel="stylesheet" href="/ananke/css/main.min.css" >



<link rel="stylesheet" href="/css/social-share.css">



  
    <link rel="stylesheet" href="/css/custom.css">
  

  
    <link rel="stylesheet" href="/css/article-enhancements.css">
  

    

<script src="/js/social-share.js"></script>



    
      

    

    

    
      <link rel="canonical" href="http://192.168.50.247:1313/insights/llmagi-20250704151004340-3/">
    

    <meta property="og:url" content="http://192.168.50.247:1313/insights/llmagi-20250704151004340-3/">
  <meta property="og:site_name" content="AI内参">
  <meta property="og:title" content="超越符号：杨立昆新研究揭示LLM认知鸿沟，预示AGI之路范式巨变">
  <meta property="og:description" content="杨立昆的最新研究量化揭示了LLM与人类认知策略的根本差异：LLM擅长统计压缩，而人类侧重适应性理解，预示着单纯扩大模型规模无法实现通用人工智能。文章深入探讨了强化学习、大型概念模型和世界模型等多元化新路径，指出AI发展将从单一的预训练范式转向多模态、物理世界锚定与架构创新相结合，以期弥合认知鸿沟，迈向更具理解力的通用智能。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="insights">
    <meta property="article:published_time" content="2025-07-04T15:10:04+08:00">
    <meta property="article:modified_time" content="2025-07-04T15:10:04+08:00">
    <meta property="article:tag" content="LLM局限">
    <meta property="article:tag" content="AGI路径">
    <meta property="article:tag" content="认知鸿沟">
    <meta property="article:tag" content="世界模型">
    <meta property="article:tag" content="大模型架构">

  <meta itemprop="name" content="超越符号：杨立昆新研究揭示LLM认知鸿沟，预示AGI之路范式巨变">
  <meta itemprop="description" content="杨立昆的最新研究量化揭示了LLM与人类认知策略的根本差异：LLM擅长统计压缩，而人类侧重适应性理解，预示着单纯扩大模型规模无法实现通用人工智能。文章深入探讨了强化学习、大型概念模型和世界模型等多元化新路径，指出AI发展将从单一的预训练范式转向多模态、物理世界锚定与架构创新相结合，以期弥合认知鸿沟，迈向更具理解力的通用智能。">
  <meta itemprop="datePublished" content="2025-07-04T15:10:04+08:00">
  <meta itemprop="dateModified" content="2025-07-04T15:10:04+08:00">
  <meta itemprop="wordCount" content="54">
  <meta itemprop="keywords" content="LLM局限,AGI路径,认知鸿沟,世界模型,大模型架构">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="超越符号：杨立昆新研究揭示LLM认知鸿沟，预示AGI之路范式巨变">
  <meta name="twitter:description" content="杨立昆的最新研究量化揭示了LLM与人类认知策略的根本差异：LLM擅长统计压缩，而人类侧重适应性理解，预示着单纯扩大模型规模无法实现通用人工智能。文章深入探讨了强化学习、大型概念模型和世界模型等多元化新路径，指出AI发展将从单一的预训练范式转向多模态、物理世界锚定与架构创新相结合，以期弥合认知鸿沟，迈向更具理解力的通用智能。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
  
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://192.168.50.247:1313/newsimages/selected_image_YYYY-07-Jul%204,%202025_15-05-16-293.jpg');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        <img src="/logo/logo.png" class="w100 mw5-ns" alt="AI内参" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="">
              首页
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/insights/" title="">
              洞察
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/morningnews/" title="">
              早报
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/newspaper/" title="">
              日报
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/explore/" title="">
              主题探索
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="">
              关于
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref helvetica tracked">
        <span style="color: #999;">2025-07-04 15:10</span>
      </aside><div id="sharing" class="mt3 ananke-socials"><a href="https://twitter.com/intent/tweet/?text=%E8%B6%85%E8%B6%8A%E7%AC%A6%E5%8F%B7%EF%BC%9A%E6%9D%A8%E7%AB%8B%E6%98%86%E6%96%B0%E7%A0%94%E7%A9%B6%E6%8F%AD%E7%A4%BALLM%E8%AE%A4%E7%9F%A5%E9%B8%BF%E6%B2%9F%EF%BC%8C%E9%A2%84%E7%A4%BAAGI%E4%B9%8B%E8%B7%AF%E8%8C%83%E5%BC%8F%E5%B7%A8%E5%8F%98%20-%20%E6%9D%A8%E7%AB%8B%E6%98%86%E7%9A%84%E6%9C%80%E6%96%B0%E7%A0%94%E7%A9%B6%E9%87%8F%E5%8C%96%E6%8F%AD%E7%A4%BA%E4%BA%86LLM%E4%B8%8E%E4%BA%BA%E7%B1%BB%E8%AE%A4%E7%9F%A5%E7%AD%96%E7%95%A5%E7%9A%84%E6%A0%B9%E6%9C%AC%E5%B7%AE%E5%BC%82%EF%BC%9ALLM%E6%93%85%E9%95%BF%E7%BB%9F%E8%AE%A1%E5%8E%8B%E7%BC%A9%EF%BC%8C%E8%80%8C%E4%BA%BA%E7%B1%BB%E4%BE%A7%E9%87%8D%E9%80%82%E5%BA%94%E6%80%A7%E7%90%86%E8%A7%A3%EF%BC%8C%E9%A2%84%E7%A4%BA%E7%9D%80%E5%8D%95%E7%BA%AF%E6%89%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E6%97%A0%E6%B3%95%E5%AE%9E%E7%8E%B0%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%82%E6%96%87%E7%AB%A0%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%AE%A8%E4%BA%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%81%E5%A4%A7%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%E7%AD%89%E5%A4%9A%E5%85%83%E5%8C%96%E6%96%B0%E8%B7%AF%E5%BE%84%EF%BC%8C%E6%8C%87%E5%87%BAAI%E5%8F%91%E5%B1%95%E5%B0%86%E4%BB%8E%E5%8D%95%E4%B8%80%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E8%8C%83%E5%BC%8F%E8%BD%AC%E5%90%91%E5%A4%9A%E6%A8%A1%E6%80%81%E3%80%81%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E9%94%9A%E5%AE%9A%E4%B8%8E%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0%E7%9B%B8%E7%BB%93%E5%90%88%EF%BC%8C%E4%BB%A5%E6%9C%9F%E5%BC%A5%E5%90%88%E8%AE%A4%E7%9F%A5%E9%B8%BF%E6%B2%9F%EF%BC%8C%E8%BF%88%E5%90%91%E6%9B%B4%E5%85%B7%E7%90%86%E8%A7%A3%E5%8A%9B%E7%9A%84%E9%80%9A%E7%94%A8%E6%99%BA%E8%83%BD%E3%80%82&amp;amp;url=http%3A%2F%2F192.168.50.247%3A1313%2Finsights%2Fllmagi-20250704151004340-3%2F"
          class="ananke-social-link x-twitter no-underline"
          title="Share on X" aria-label="Share on X"
          target="_blank" rel="nofollow noopener noreferrer"><span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
                
              </span></a><a href="http://service.weibo.com/share/share.php?title=%E8%B6%85%E8%B6%8A%E7%AC%A6%E5%8F%B7%EF%BC%9A%E6%9D%A8%E7%AB%8B%E6%98%86%E6%96%B0%E7%A0%94%E7%A9%B6%E6%8F%AD%E7%A4%BALLM%E8%AE%A4%E7%9F%A5%E9%B8%BF%E6%B2%9F%EF%BC%8C%E9%A2%84%E7%A4%BAAGI%E4%B9%8B%E8%B7%AF%E8%8C%83%E5%BC%8F%E5%B7%A8%E5%8F%98%20-%20%E6%9D%A8%E7%AB%8B%E6%98%86%E7%9A%84%E6%9C%80%E6%96%B0%E7%A0%94%E7%A9%B6%E9%87%8F%E5%8C%96%E6%8F%AD%E7%A4%BA%E4%BA%86LLM%E4%B8%8E%E4%BA%BA%E7%B1%BB%E8%AE%A4%E7%9F%A5%E7%AD%96%E7%95%A5%E7%9A%84%E6%A0%B9%E6%9C%AC%E5%B7%AE%E5%BC%82%EF%BC%9ALLM%E6%93%85%E9%95%BF%E7%BB%9F%E8%AE%A1%E5%8E%8B%E7%BC%A9%EF%BC%8C%E8%80%8C%E4%BA%BA%E7%B1%BB%E4%BE%A7%E9%87%8D%E9%80%82%E5%BA%94%E6%80%A7%E7%90%86%E8%A7%A3%EF%BC%8C%E9%A2%84%E7%A4%BA%E7%9D%80%E5%8D%95%E7%BA%AF%E6%89%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E6%97%A0%E6%B3%95%E5%AE%9E%E7%8E%B0%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%82%E6%96%87%E7%AB%A0%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%AE%A8%E4%BA%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%81%E5%A4%A7%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%E7%AD%89%E5%A4%9A%E5%85%83%E5%8C%96%E6%96%B0%E8%B7%AF%E5%BE%84%EF%BC%8C%E6%8C%87%E5%87%BAAI%E5%8F%91%E5%B1%95%E5%B0%86%E4%BB%8E%E5%8D%95%E4%B8%80%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E8%8C%83%E5%BC%8F%E8%BD%AC%E5%90%91%E5%A4%9A%E6%A8%A1%E6%80%81%E3%80%81%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E9%94%9A%E5%AE%9A%E4%B8%8E%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0%E7%9B%B8%E7%BB%93%E5%90%88%EF%BC%8C%E4%BB%A5%E6%9C%9F%E5%BC%A5%E5%90%88%E8%AE%A4%E7%9F%A5%E9%B8%BF%E6%B2%9F%EF%BC%8C%E8%BF%88%E5%90%91%E6%9B%B4%E5%85%B7%E7%90%86%E8%A7%A3%E5%8A%9B%E7%9A%84%E9%80%9A%E7%94%A8%E6%99%BA%E8%83%BD%E3%80%82&amp;amp;url=http%3A%2F%2F192.168.50.247%3A1313%2Finsights%2Fllmagi-20250704151004340-3%2F"
          class="ananke-social-link weibo no-underline"
          title="Share on Weibo" aria-label="Share on Weibo"
          target="_blank" rel="nofollow noopener noreferrer"><span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg>
                
              </span></a><a href="javascript:void(0)" onclick="showWeixinQR()"
          class="ananke-social-link weixin no-underline"
          title="Share on Weixin" aria-label="Share on Weixin"><span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154zm-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4zm-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2zM563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4zm-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6zm107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6z"/></svg>
                
              </span></a></div><h1 class="f1 athelas mt3 mb1">超越符号：杨立昆新研究揭示LLM认知鸿沟，预示AGI之路范式巨变</h1>
      
      <p class="tracked"><strong>温故智新AIGC实验室</strong>
      </p>
      
      
    </header>
    <div class="article-content nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>TL;DR：</p>
<blockquote>
<p>杨立昆等人的最新研究通过量化分析，揭示了当前大型语言模型（LLM）与人类在认知策略上的根本性差异，即LLM擅长统计压缩，而人类则追求适应性与功能性理解。这表明单纯扩大模型规模的“缩放定律”无法弥合这一认知鸿沟，预示着通往通用人工智能（AGI）的路径正从单一的预训练模型转向多模态、世界模型及架构革新等多元化范式。</p></blockquote>
<p>长期以来，Meta首席AI科学家杨立昆（Yann LeCun）对大型语言模型（LLM）的技术路线抱持着深刻的怀疑。他认为，以预测下一个词为核心的自回归模型，即便规模无限扩大，也无法孕育出真正的智能，因为它本质上无法实现人类那般深刻的理解、推理能力。这一观点曾被视为学界“派系之争”的产物，缺乏直接的实证支撑。然而，随着JEPA 2论文的发布及其优异表现，以及他共同署名的重量级新研究《从toekn到思想：LLM与人类如何在压缩与意义之间权衡》（From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning）<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>的面世，杨立昆的批判终于获得了坚实的理论支撑。这项研究不仅仅是对LLM能力边界的审视，更是对当前人工智能范式底层基础的一次深刻挑战，预示着通往AGI的道路可能需要根本性的转向。</p>
<h3 id="技术原理与创新点解析">技术原理与创新点解析</h3>
<p>这项突破性研究的核心在于将一个看似哲学层面的问题——“机器的理解与人类的理解有何不同？”——转化为了一个可以量化的科学问题。研究者并未直接定义“理解”，而是巧妙地选择衡量“理解”背后的信息组织策略。他们设计了一个名为“认知效率计分器”（L score）的工具，用于评估任何智能系统（无论是人脑还是AI）在组织信息时，如何在信息的极致压缩（Complexity）与意义的忠实保留（Distortion）之间取得平衡。一个理想的系统，其L分数应尽可能低，意味着它能以最经济的方式最大程度地保留事物原意。</p>
<p>研究团队通过三个精心设计的实验，量化测量了人脑和LLM之间的认知差距：</p>
<ol>
<li>
<p><strong>抽象概念形成：惊人的一致性与表象的迷惑</strong>：
第一个实验从宏观层面观察LLM自发形成的概念类别，在整体结构上与人类的分类习惯是否相似。结果显示，包括Llama、Gemma、Qwen、Phi和Mistral在内的多系列LLM，其词嵌入聚类结果与人类对“水果”、“家具”等概念的分类惊人一致，显著高于随机水平。这似乎证明了LLM并非简单的“随机鹦鹉”，它们确实从海量文本数据中习得了深刻的语义关联。其中，Bert模型表现出与人类最接近的聚类结果<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
</li>
<li>
<p><strong>类别内部精细语义：原型认知的缺失</strong>：
然而，表面的相似无法掩盖深层差异。第二个实验深入到每个类别的内部，探究LLM是否能理解精细语义结构，例如“典型性”。对人类而言，“麻雀”是比“鸵鸟”更典型的“鸟”，这种判断源于我们丰富的多模态现实经验。但研究发现，LLM的内部表征虽然能将麻雀和企鹅聚在一起，却无法稳定地反映出前者更具代表性的语义细节。在LLM看来，一个类别内的所有成员更像是地位相对平等的点，缺乏人类认知中那种强烈的“原型”或“范例”结构。</p>
</li>
<li>
<p><strong>核心认知策略：压缩大师与适应大师的本质分歧</strong>：
第三个实验揭示了两种智能在面对“压缩vs.意义”这一根本性权衡时的不同策略。当人类的分类数据和所有LLM的聚类结果被代入统一的“效率计分器”(L) 后，结果清晰展现：所有LLM，无论规模大小，都获得了极低的L分数，是天生的“效率之王”，它们驱动自己寻找最优的统计压缩方案。而人类的认知数据则获得了显著更高的L分数，在纯粹的统计效率竞赛中“惨败”。</p>
</li>
</ol>
<p>这一洞见尤为深刻：人类认知系统中的这种“低效”，并非缺陷，而是其强大功能的体现。我们的大脑并非为成为完美的压缩软件而进化，其首要任务是在复杂、动态、不确定性的真实世界中生存和繁衍。因此，我们的概念系统必须是灵活、丰富、可塑的，能够支持复杂的因果推理、功能判断和有效的社会沟通。这种为“适应性”而保留的“冗余”和“模糊性”，在纯粹的统计计分器上自然表现为“低效”。<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<h3 id="llm核心范式的深层局限">LLM核心范式的深层局限</h3>
<p>这项研究最令人警醒的结论是：<strong>传统的“缩放定律”（Scaling Law）在弥合LLM与人类认知鸿沟方面可能完全失效。</strong> 论文明确指出，在“与人类概念分类对齐”的任务上，并非模型越大就做得越好。例如，相对较小的BERT-large模型（约3.4亿参数）的表现，常常与大得多的解码器模型不相上下，甚至超越它们。在衡量对齐度的图表中，性能点（AMI分数）是分散的，并未随着模型尺寸（从5亿到700亿参数）的增加而呈现出清晰的、持续上升的曲线。<sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>这意味着，单纯增加参数量并不能保证模型能更好地抓住人类概念的深层结构。正如杨立昆所言，LLM和人类玩的游戏规则完全不同：一个是“压缩猛兽”，一个是“适应性猎手”。仅仅为这头“压缩猛兽”喂更多的食物（增加参数量），只会让它长得更大、更强壮，但并不会让它进化成“适应性猎手”。<strong>物种的“基因”（即模型架构和训练范式）决定了它的基本生存策略。</strong> 这种根本性的机制差异，而非规模，才是LLM无法跨越认知鸿沟的症结所在。</p>
<h3 id="产业生态的范式演进">产业生态的范式演进</h3>
<p>那么，这项研究是否意味着以GPT系列为代表的当前大型语言模型技术路线已被宣判“死刑”？答案或许是否定的，但那个单一、庞大、试图包揽一切的“预训练Scaling神话”时代，可能正在迎来它的终局。从投资逻辑和产业布局来看，仅靠堆砌算力和数据来追求模型“大而全”的策略，其边际效益正在递减，并且无法解决核心的认知缺陷。<strong>未来的投资和研发重心将从“规模化扩张”转向“能力深度化”和“结构创新化”。</strong></p>
<p>对于依赖LLM的企业级应用开发者而言，这意味着对现有模型能力的认识需要更加清晰，对于期望AI能真正“理解”复杂业务逻辑和语境的需求，单一LLM的解决方案可能不足以支撑。这会催生对AI系统集成、多模态融合以及结合外部知识和推理引擎的更复杂需求，从而为<strong>定制化AI解决方案</strong>和<strong>AI赋能服务</strong>带来新的市场机遇。</p>
<h3 id="通往通用智能的新航向">通往通用智能的新航向</h3>
<p>面对LLM的固有局限，研究和产业界正探索多条路径，以期破除瓶颈，迈向更接近人类智能的通用能力：</p>
<ol>
<li>
<p><strong>“软件层面”的精细调教：引入更丰富的奖励信号。</strong> 这是当前业界投入最多且最接近现实应用的改良方案，核心思想是通过强化学习中的奖励模型来引导LLM行为。通过设计极其精密的奖励机制，鼓励模型识别并解释概念的“典型性”，构建因果推理链条，甚至表达知识局限性。尽管杨立昆的实验采用的是非推理型模型，但当前强化学习（如RLHF）在提升模型行为对齐方面已展现出显著效果，它至少作为一种“补丁”机制，在短期内提升了LLM的“可用性”和“拟人性”。然而，这种方法能否从根本上改变其“统计压缩”的内在表征策略，仍有待更深入的验证。</p>
</li>
<li>
<p><strong>“硬件层面”的架构革新：大型概念模型（LCMs）。</strong> 这是更激进的革命性路径，旨在从根本上改变自回归模型的生成粒度。Meta提出的LCMs框架便是一个绝佳例证，它将生成单位从预测下一个“词”（Token）跃升到预测下一个“概念”（Concept）。LCMs通过构建一个双系统架构：一个扮演“系统二”（规划器/思考者）的“概念规划模型”，负责在抽象概念空间中进行深思熟虑的逻辑规划和因果链构建；另一个扮演“系统一”（执行器）的“文本实现模型”，接收概念向量指令并高效生成连贯文本。<strong>这种从架构层面对“深思熟虑”的内在要求，有望让模型超越单一的“统计压缩”目标，实现人类认知所拥有的更广泛功能性需求。</strong></p>
</li>
<li>
<p><strong>杨立昆的“世界模型”之路：多模态地基与物理世界的锚定。</strong> 这条路径让LLM走出纯粹的文本“洞穴”，去拥抱一个由图像、声音和物理规律构成的、多姿多彩的真实世界。其核心在于<strong>多模态地基（Multi-modal Grounding）</strong> 和<strong>世界模型（World Models）</strong>。当前LLM的知识是“悬浮”的，缺乏现实世界的“锚点”。人类之所以能识别“典型性”，是因为我们的概念是由“丰富的、多方面的标准（如感知属性、功能角色）”共同定义的。因此，为AI接上“感官”——多模态学习，是第一步。第二步，世界模型的优化目标与LLM完全不同，它首要任务是“如何最准确地预测真实世界的下一步”。为了预测玻璃杯掉落会碎而非弹起，模型必须保留关于“玻璃”易碎性和“地面”坚硬性的丰富物理属性信息。这些在纯文本压缩中被视为“噪声”的细节，在预测现实世界的任务中却是至关重要的核心信号，从而迫使模型构建更丰富、更细致、更接近物理现实的内部表征，自然摆脱“过度压缩”陷阱。</p>
</li>
</ol>
<h3 id="技术与文明的未来交汇">技术与文明的未来交汇</h3>
<p>杨立昆的这项研究，无疑为人工智能的未来发展路径投下了深远的启示。它提醒我们，AGI的实现并非线性地叠加算力和数据，而是需要一场深刻的范式变革，从“符号到思想”的跃迁，需要AI系统真正理解世界的内在机制和因果关系。</p>
<p>这不仅是一场技术路线的争论，更是一次对“智能”本质的哲学追问。我们能否设计出能够像人类一样，在效率与适应性、压缩与意义之间做出精妙权衡的机器？这不仅关乎AI的能力边界，也触及人类自身认知的独特性。未来的旅程，不再是简单地为这个聪明的“缸中之脑”提供更多、更复杂的文本食粮，而是要引导它慢慢长出眼睛、耳朵和双手，让它在与真实世界的互动中，在对物理规律和因果关系的亲身体验中，真正理解“从符号到思想”的深刻含义。<strong>这可能意味着，通用人工智能的未来将不再是单一模型的胜利，而是多模态融合、架构创新与世界模型协同作用的复杂系统工程。</strong> 最终，AI有望从一个强大的工具，蜕变为一个能与我们共情、共存、共同创造的伙伴，深刻影响人类文明的进程。</p>
<h2 id="引用">引用</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://m.36kr.com/p/3364112069871367">预训练通往AGI之路已死？杨立昆揭示了LLM无法跨越的认知鸿沟</a>·36氪·郝博阳（2025/7/4）·检索日期2025/7/4&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/llm%E5%B1%80%E9%99%90/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">LLM局限</a>
   </li>
  
   <li class="list di">
     <a href="/tags/agi%E8%B7%AF%E5%BE%84/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">AGI路径</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E8%AE%A4%E7%9F%A5%E9%B8%BF%E6%B2%9F/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">认知鸿沟</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">世界模型</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">大模型架构</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3" style="color: #666;">相关阅读</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/insights/article-20250703214006827-1/">AI编程范式革命：从“副驾驶”到“自动驾驶”，重塑软件创造边界</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/agi-20250703174003934-0/">从像素到世界：李飞飞的“空间智能”远征，定义AGI新版图</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/newspaper/2025-06-23-06-23-ai-/">06-23日报| AI的“觉醒”：从操控数字社会到自我保命，人类正走向“数字失控”？</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250623113258999-15/">构建数字社会：揭秘AI世界模型中的选举与共存实验</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250623113044285-15/">构建数字社会：揭秘AI世界模型中的选举与共存实验</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/mewmai-20250619112004625-6/">医学世界模型MeWM：AI如何让医生“预演”疾病，开启精准医疗新纪元</a>
        </li>
	    
    </ul>
</div>



<div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links mt3">
  <p class="f5 b mb3" style="color: #666;">AI写作声明</p>
  <div class="f6 lh-copy" style="color: #777;">
    <p>本文由AI系统全流程生成，涵盖选题、资料整合与撰写。文章发布后由人工编辑进行核查。文末附有详细的[引用信息索引]，供您查证溯源。</p>
  </div>
</div></aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://192.168.50.247:1313/" >
    &copy;  AI内参 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

    <script src="/js/relative-time.js"></script>
  </body>
</html>
