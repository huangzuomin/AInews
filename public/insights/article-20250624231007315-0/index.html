<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理 | AI内参</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">
    <meta name="generator" content="Hugo 0.147.8">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    <meta name="author" content="温故智新AIGC实验室">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  
    <link rel="stylesheet" href="/css/custom.css">
  

  
    <link rel="stylesheet" href="/css/article-enhancements.css">
  


    


    
      

    

    

    
      <link rel="canonical" href="http://192.168.50.247:1313/insights/article-20250624231007315-0/">
    

    <meta property="og:url" content="http://192.168.50.247:1313/insights/article-20250624231007315-0/">
  <meta property="og:site_name" content="AI内参">
  <meta property="og:title" content="边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理">
  <meta property="og:description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="insights">
    <meta property="article:published_time" content="2025-06-24T23:10:07+08:00">
    <meta property="article:modified_time" content="2025-06-24T23:10:07+08:00">
    <meta property="article:tag" content="端侧AI">
    <meta property="article:tag" content="大模型推理">
    <meta property="article:tag" content="小米小爱同学">
    <meta property="article:tag" content="资源受限">
    <meta property="article:tag" content="LoRA">
    <meta property="article:tag" content="投机推理">

  <meta itemprop="name" content="边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理">
  <meta itemprop="description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">
  <meta itemprop="datePublished" content="2025-06-24T23:10:07+08:00">
  <meta itemprop="dateModified" content="2025-06-24T23:10:07+08:00">
  <meta itemprop="wordCount" content="37">
  <meta itemprop="keywords" content="端侧AI,大模型推理,小米小爱同学,资源受限,LoRA,投机推理,共享基座模型,Linear Attention,边缘计算,AICon">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理">
  <meta name="twitter:description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
  
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://192.168.50.247:1313/images/default%20%2811%29.png');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        <img src="/logo/logo.png" class="w100 mw5-ns" alt="AI内参" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="">
              首页
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/insights/" title="">
              洞察
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/newspaper/" title="">
              日报
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/explore/" title="">
              主题探索
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="">
              关于
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref helvetica tracked">
        <span style="color: #999;">2025-06-24 23:10</span>
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理</h1>
      
      <p class="tracked"><strong>温故智新AIGC实验室</strong>
      </p>
      
      
    </header>
    <div class="article-content nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><blockquote>
<p>在资源高度受限的移动和物联网设备上，小米小爱同学团队通过自研推理框架，实现了大模型每秒180个token的实时推理速度，并借助LoRA插件化与共享基座模型，有效解决了多业务并发与内存占用难题。这一系列工程创新不仅突破了端侧AI部署的技术瓶颈，更预示着通用人工智能普惠化的未来图景。</p></blockquote>
<p>在人工智能的宏大叙事中，一个关键的分水岭正在出现：计算智能正从庞大的数据中心向我们手中的每一个设备延伸。将大型语言模型（LLMs）——这些拥有数十亿参数、能力惊人的数字大脑——部署到智能手机、汽车和物联网设备等边缘端，是实现AI无处不在的关键一步。然而，这并非易事。这些设备所面临的严苛资源限制，无论是计算能力、内存空间、功耗，还是模型更新机制的灵活性，都构成了巨大的工程挑战。小米小爱同学团队作为这一领域的先行者，正在通过一系列深思熟虑的技术策略，突破这些看似难以逾越的障碍。</p>
<h3 id="突破边缘限制小米的工程智慧">突破边缘限制：小米的工程智慧</h3>
<p>小米小爱同学端侧AI负责人杨永杰在近期的一次访谈中指出，尽管端侧大模型被广泛视为未来的重要方向，但其商业化落地进程相对缓慢，核心症结在于端侧设备固有的<strong>资源限制</strong>和大模型<strong>快速迭代</strong>的特性<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。云端大模型可以轻松承载数十甚至数百亿参数，且能快速更新。但端侧设备仅能支持参数量约40亿（4B）的模型，且即使经过低比特量化，也可能牺牲模型效果。此外，端侧模型更新机制滞后，难以跟上云端日新月异的迭代速度。杨永杰认为，目前的端侧大模型更像是“技术积累”，为未来硬件和模型成熟做准备<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>面对这些挑战，小米团队选择了一条更为艰难但可能更具前景的道路：<strong>自研大模型推理框架</strong>。市场上针对端侧大模型的开源框架稀缺，尤其在NPU（神经网络处理器）上，由于芯片厂商通常不开放接口，导致NPU推理框架往往只能由厂商自行开发。这使得云端框架如vLLM或SGLang的丰富优化手段难以直接移植到端侧。小米的全栈自研，正是为了实现对每一个模块的细致性能优化，并能借鉴云端经验进行深度适配<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>他们的努力取得了显著成果：实现了<strong>180 tokens/s</strong>的实时推理性能<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。这一速度在资源受限的端侧设备上堪称卓越，其背后凝结着多项系统级与模型级优化策略：</p>
<ul>
<li><strong>动态输入与动态上下文支持</strong>：传统NPU常采用静态图，要求固定输入尺寸，导致资源浪费。小米的框架通过自动切分输入尺寸，让模型在保持静态图性能的同时支持动态输入，大幅提升了资源利用率和吞吐率。</li>
<li><strong>投机推理（Speculative Decoding）优化</strong>：这项技术在云端通常能带来2-3倍的加速，而小米通过自研策略在端侧实现了高达<strong>7-10倍的解码加速</strong>。这意味着原本可能只有20 tokens/s的速度，能够提升到足以媲美部分云端场景的200 tokens/s。</li>
<li><strong>量化与指令级优化</strong>：通过对模型进行低比特量化，以及利用CPU的Neon指令集对量化、反量化、采样等关键操作进行加速，进一步榨取硬件性能。</li>
</ul>
<p>这些工程实践不仅验证了端侧大模型的高性能潜力，也为车载等多模态应用场景的落地奠定了基础<sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<h3 id="架构创新多任务并发与资源复用">架构创新：多任务并发与资源复用</h3>
<p>小爱同学作为一款语音助手，需要承载语音控制、多轮对话、智能家居等多种任务。虽然目前单个请求链路通常是串行的，但模型能力会被其他业务共用，导致业务间可能出现并发冲突。更深层次的挑战在于，端侧设备的NPU通常<strong>不支持并发推理</strong>，其硬件设计以串行执行为主。试图通过_multi-batch_提升并发效率在端侧也收效甚微，因为单条请求本身就可能已接近设备算力的上限<sup id="fnref4:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>为了在有限的存储空间和内存下支持多任务、多业务场景，小米团队采用了**“共享基座模型 + 插件化能力”**的创新架构。例如，一台12GB内存的手机部署一个4B大模型可能就需要3GB内存，实际可分配给大模型的空间可能更少。在这种背景下，为每个业务部署独立模型是不可行的。</p>
<p>小米的解决方案是：选择一个统一的基础大模型作为“基座”，然后针对不同的业务单独训练轻量级的<strong>LoRA（Low-Rank Adaptation）模块</strong>。当A业务的请求到来时，加载基座模型与A的LoRA进行推理；B业务请求到来时，则卸载A的LoRA，换上B的LoRA。这种机制在只保留一个基础模型的前提下，通过LoRA的插件化和轻量化特性，实现了在资源有限的设备上<strong>动态切换不同业务能力</strong>，从而支持多个任务的并发调用<sup id="fnref5:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。这种“参数共享 + 差异定制”的模式，在内存利用率和扩展能力上都显示出显著优势。</p>
<p>在跨芯片平台部署方面，小米的推理框架采用了模块化、后端解耦的设计，与传统模型框架处理多后端问题异曲同工。杨永杰指出，大模型虽然规模庞大，但在框架设计层面，与底层硬件的绑定程度反而没有传统模型那么深，使得抽象出通用接口、实现跨平台迁移变得相对容易<sup id="fnref6:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>性能优化策略的组合与优先级也是工程实践中的关键。小米团队倾向于优先实现那些技术价值大、适用面广、且与其他手段不冲突的优化方式，例如<strong>并行解码、低比特量化、带宽控制</strong>等都可以组合使用。对于并非所有业务都需要的优化（如_prompt cache_），则将其封装为业务可选的配置项，降低使用复杂度，同时提供针对性的性能提升建议<sup id="fnref7:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<h3 id="展望未来算力与模型架构的双轮驱动">展望未来：算力与模型架构的双轮驱动</h3>
<p>在杨永杰看来，端侧大模型未来最具突破性的方向将集中在两大核心目标：</p>
<ol>
<li><strong>面向大模型优化的硬件能力提升</strong>：当前端侧大模型的诸多限制，如功耗高、挤占其他业务资源导致系统卡顿，都直接源于硬件算力不足。如同当年传统AI模型兴起催生了NPU，杨永杰预计，随着大模型热度持续，新一代面向大模型的端侧芯片将应运而生。一旦这类硬件普及，端侧模型的能力将得到大幅增强，从而让更多业务真正落地，实现商业化<sup id="fnref8:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
<li><strong>模型架构的演进</strong>：目前主流大模型多基于Transformer架构，其自回归特性导致当上下文（context）变长时，资源占用（特别是KV cache）会显著增加，内存和算力压力陡增。业界正在探索的<strong>Linear Attention</strong>架构，如<strong>Mamba、RWKV</strong>等，尝试在保持模型能力的同时，使内存使用与输入长度无关。这项技术对于资源敏感的端侧场景至关重要，尤其是在多模态任务中，图片、视频等非文本输入会迅速拉长上下文，Transformer的瓶颈将愈发凸显。尽管Linear Attention目前整体效果尚不及Transformer，但其研究热度与潜力预示着未来实用级别突破的可能性<sup id="fnref9:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
</ol>
<p>小米小爱同学团队的实践，无疑为全球范围内的边缘AI部署提供了宝贵的经验和范例。这不仅仅是技术性能的简单提升，更是对AI普惠化未来的一次深度探索。当AI不再局限于云端，而是真正融入我们日常使用的每一个设备时，它将以更低的延迟、更高的隐私保护、更强大的离线能力，深刻改变我们与数字世界的互动方式。</p>
<h2 id="引文">引文</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://www.infoq.cn/article/ifffkybhkhafkxdi4iac" target="_blank">小爱同学在高性能端侧大模型推理的实践｜AICon北京</a>·InfoQ·罗燕珊（2025/6/24）·检索日期2025/6/24&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref9:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://www.163.com/dy/article/K2R51P7D0511D3QS.html" target="_blank">小米小爱同学：资源受限下，实现端侧大模型的高性能推理</a>·网易（2025/6/24）·检索日期2025/6/24&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/%E7%AB%AF%E4%BE%A7ai/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">端侧AI</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">大模型推理</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%B0%8F%E7%B1%B3%E5%B0%8F%E7%88%B1%E5%90%8C%E5%AD%A6/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">小米小爱同学</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E8%B5%84%E6%BA%90%E5%8F%97%E9%99%90/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">资源受限</a>
   </li>
  
   <li class="list di">
     <a href="/tags/lora/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">LoRA</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%8A%95%E6%9C%BA%E6%8E%A8%E7%90%86/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">投机推理</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%85%B1%E4%BA%AB%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">共享基座模型</a>
   </li>
  
   <li class="list di">
     <a href="/tags/linear-attention/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Linear Attention</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">边缘计算</a>
   </li>
  
   <li class="list di">
     <a href="/tags/aicon/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">AICon</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3" style="color: #666;">相关阅读</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/insights/article-20250624101004329-0/">AI手机核心之争：芯片巨头如何在性能、架构与生态中角逐未来</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250623181004317-3/">更深层次的变革：AI如何重塑软件开发的起点、过程与目的</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250620161004296-1/">大模型幻觉之殇与协同之光：智能投顾如何精准破局</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250620111004334-2/">黑芝麻智能的战略棋局：低功耗芯片如何加速具身智能浪潮</a>
        </li>
	    
    </ul>
</div>



<div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links mt3">
  <p class="f5 b mb3" style="color: #666;">AI写作声明</p>
  <div class="f6 lh-copy" style="color: #777;">
    <p>本文由AI系统全流程生成，涵盖选题、资料整合与撰写。文章发布后由人工编辑进行核查。文末附有详细的[引用信息索引]，供您查证溯源。</p>
  </div>
</div></aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://192.168.50.247:1313/" >
    &copy;  AI内参 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

    <script src="/js/relative-time.js"></script>
  </body>
</html>
