<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1315&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理 | AI内参</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">
    <meta name="generator" content="Hugo 0.147.0">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    <meta name="author" content="温故智新AIGC实验室">
    

    


<link rel="stylesheet" href="/ananke/css/main.min.css" >



<link rel="stylesheet" href="/css/social-share.css">



  
    <link rel="stylesheet" href="/css/custom.css">
  

  
    <link rel="stylesheet" href="/css/article-enhancements.css">
  

    

<script src="/js/social-share.js"></script>



    
      

    

    

    
      <link rel="canonical" href="http://localhost:1315/insights/article-20250624231007315-0/">
    

    <meta property="og:url" content="http://localhost:1315/insights/article-20250624231007315-0/">
  <meta property="og:site_name" content="AI内参">
  <meta property="og:title" content="边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理">
  <meta property="og:description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="insights">
    <meta property="article:published_time" content="2025-06-24T23:10:07+08:00">
    <meta property="article:modified_time" content="2025-06-24T23:10:07+08:00">
    <meta property="article:tag" content="端侧AI">
    <meta property="article:tag" content="大模型推理">
    <meta property="article:tag" content="小米小爱同学">
    <meta property="article:tag" content="资源受限">
    <meta property="article:tag" content="LoRA">
    <meta property="article:tag" content="投机推理">

  <meta itemprop="name" content="边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理">
  <meta itemprop="description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">
  <meta itemprop="datePublished" content="2025-06-24T23:10:07+08:00">
  <meta itemprop="dateModified" content="2025-06-24T23:10:07+08:00">
  <meta itemprop="wordCount" content="37">
  <meta itemprop="keywords" content="端侧AI,大模型推理,小米小爱同学,资源受限,LoRA,投机推理,共享基座模型,Linear Attention,边缘计算,AICon">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理">
  <meta name="twitter:description" content="小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座&#43;LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
  
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://localhost:1315/images/default%20%2811%29.png');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        <img src="/logo/logo.png" class="w100 mw5-ns" alt="AI内参" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="">
              首页
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/insights/" title="">
              洞察
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/morningnews/" title="">
              早报
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/newspaper/" title="">
              日报
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/explore/" title="">
              主题探索
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="">
              关于
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref helvetica tracked">
        <span style="color: #999;">2025-06-24 23:10</span>
      </aside><div id="sharing" class="mt3 ananke-socials"><a href="https://twitter.com/intent/tweet/?text=%E8%BE%B9%E7%BC%98%E6%99%BA%E8%83%BD%E7%9A%84%E7%AA%81%E7%A0%B4%EF%BC%9A%E5%B0%8F%E7%B1%B3%E5%B0%8F%E7%88%B1%E5%90%8C%E5%AD%A6%E5%A6%82%E4%BD%95%E5%9C%A8%E8%B5%84%E6%BA%90%E5%8F%97%E9%99%90%E4%B8%8B%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%20-%20%E5%B0%8F%E7%B1%B3%E5%B0%8F%E7%88%B1%E5%90%8C%E5%AD%A6%E5%9B%A2%E9%98%9F%E5%9C%A8%E7%AB%AF%E4%BE%A7%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%96%B9%E9%9D%A2%E5%8F%96%E5%BE%97%E4%BA%86%E6%98%BE%E8%91%97%E8%BF%9B%E5%B1%95%EF%BC%8C%E9%80%9A%E8%BF%87%E8%87%AA%E7%A0%94%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%E3%80%81%E5%8A%A8%E6%80%81%E4%BC%98%E5%8C%96%E3%80%81%E6%8A%95%E6%9C%BA%E6%8E%A8%E7%90%86%E3%80%81%E9%87%8F%E5%8C%96%E4%BB%A5%E5%8F%8A%E5%88%9B%E6%96%B0%E7%9A%84%E2%80%9C%E5%85%B1%E4%BA%AB%E5%9F%BA%E5%BA%A7%2BLoRA%E2%80%9D%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%88%90%E5%8A%9F%E5%85%8B%E6%9C%8D%E4%BA%86%E7%A7%BB%E5%8A%A8%E8%AE%BE%E5%A4%87%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6%EF%BC%8C%E5%AE%9E%E7%8E%B0%E4%BA%86%E9%AB%98%E6%80%A7%E8%83%BD%E3%80%81%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%B9%B6%E5%8F%91%E3%80%82%E6%96%87%E7%AB%A0%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%BA%86%E5%B0%8F%E7%B1%B3%E7%9A%84%E6%8A%80%E6%9C%AF%E7%AD%96%E7%95%A5%EF%BC%8C%E5%B9%B6%E5%B1%95%E6%9C%9B%E4%BA%86%E6%9C%AA%E6%9D%A5%E7%A1%AC%E4%BB%B6%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%EF%BC%88%E5%A6%82Linear&amp;#43;Attention%EF%BC%89%E5%9C%A8%E6%8E%A8%E5%8A%A8%E7%AB%AF%E4%BE%A7AI%E6%99%AE%E6%83%A0%E5%8C%96%E4%B8%AD%E7%9A%84%E5%85%B3%E9%94%AE%E4%BD%9C%E7%94%A8%E3%80%82&amp;amp;url=http%3A%2F%2Flocalhost%3A1315%2Finsights%2Farticle-20250624231007315-0%2F"
          class="ananke-social-link x-twitter no-underline"
          title="Share on X" aria-label="Share on X"
          target="_blank" rel="nofollow noopener noreferrer"><span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
                
              </span></a><a href="http://service.weibo.com/share/share.php?title=%E8%BE%B9%E7%BC%98%E6%99%BA%E8%83%BD%E7%9A%84%E7%AA%81%E7%A0%B4%EF%BC%9A%E5%B0%8F%E7%B1%B3%E5%B0%8F%E7%88%B1%E5%90%8C%E5%AD%A6%E5%A6%82%E4%BD%95%E5%9C%A8%E8%B5%84%E6%BA%90%E5%8F%97%E9%99%90%E4%B8%8B%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%20-%20%E5%B0%8F%E7%B1%B3%E5%B0%8F%E7%88%B1%E5%90%8C%E5%AD%A6%E5%9B%A2%E9%98%9F%E5%9C%A8%E7%AB%AF%E4%BE%A7%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%96%B9%E9%9D%A2%E5%8F%96%E5%BE%97%E4%BA%86%E6%98%BE%E8%91%97%E8%BF%9B%E5%B1%95%EF%BC%8C%E9%80%9A%E8%BF%87%E8%87%AA%E7%A0%94%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%E3%80%81%E5%8A%A8%E6%80%81%E4%BC%98%E5%8C%96%E3%80%81%E6%8A%95%E6%9C%BA%E6%8E%A8%E7%90%86%E3%80%81%E9%87%8F%E5%8C%96%E4%BB%A5%E5%8F%8A%E5%88%9B%E6%96%B0%E7%9A%84%E2%80%9C%E5%85%B1%E4%BA%AB%E5%9F%BA%E5%BA%A7%2BLoRA%E2%80%9D%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%88%90%E5%8A%9F%E5%85%8B%E6%9C%8D%E4%BA%86%E7%A7%BB%E5%8A%A8%E8%AE%BE%E5%A4%87%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6%EF%BC%8C%E5%AE%9E%E7%8E%B0%E4%BA%86%E9%AB%98%E6%80%A7%E8%83%BD%E3%80%81%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%B9%B6%E5%8F%91%E3%80%82%E6%96%87%E7%AB%A0%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%BA%86%E5%B0%8F%E7%B1%B3%E7%9A%84%E6%8A%80%E6%9C%AF%E7%AD%96%E7%95%A5%EF%BC%8C%E5%B9%B6%E5%B1%95%E6%9C%9B%E4%BA%86%E6%9C%AA%E6%9D%A5%E7%A1%AC%E4%BB%B6%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%EF%BC%88%E5%A6%82Linear&amp;#43;Attention%EF%BC%89%E5%9C%A8%E6%8E%A8%E5%8A%A8%E7%AB%AF%E4%BE%A7AI%E6%99%AE%E6%83%A0%E5%8C%96%E4%B8%AD%E7%9A%84%E5%85%B3%E9%94%AE%E4%BD%9C%E7%94%A8%E3%80%82&amp;amp;url=http%3A%2F%2Flocalhost%3A1315%2Finsights%2Farticle-20250624231007315-0%2F"
          class="ananke-social-link weibo no-underline"
          title="Share on Weibo" aria-label="Share on Weibo"
          target="_blank" rel="nofollow noopener noreferrer"><span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg>
                
              </span></a><a href="javascript:void(0)" onclick="showWeixinQR()"
          class="ananke-social-link weixin no-underline"
          title="Share on Weixin" aria-label="Share on Weixin"><span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154zm-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4zm-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2zM563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4zm-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6zm107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6z"/></svg>
                
              </span></a></div><h1 class="f1 athelas mt3 mb1">边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理</h1>
      
      <p class="tracked"><strong>温故智新AIGC实验室</strong>
      </p>
      
      
    </header>
    <div class="article-content nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><blockquote>
<p>在资源高度受限的移动和物联网设备上，小米小爱同学团队通过自研推理框架，实现了大模型每秒180个token的实时推理速度，并借助LoRA插件化与共享基座模型，有效解决了多业务并发与内存占用难题。这一系列工程创新不仅突破了端侧AI部署的技术瓶颈，更预示着通用人工智能普惠化的未来图景。</p></blockquote>
<p>在人工智能的宏大叙事中，一个关键的分水岭正在出现：计算智能正从庞大的数据中心向我们手中的每一个设备延伸。将大型语言模型（LLMs）——这些拥有数十亿参数、能力惊人的数字大脑——部署到智能手机、汽车和物联网设备等边缘端，是实现AI无处不在的关键一步。然而，这并非易事。这些设备所面临的严苛资源限制，无论是计算能力、内存空间、功耗，还是模型更新机制的灵活性，都构成了巨大的工程挑战。小米小爱同学团队作为这一领域的先行者，正在通过一系列深思熟虑的技术策略，突破这些看似难以逾越的障碍。</p>
<h3 id="突破边缘限制小米的工程智慧">突破边缘限制：小米的工程智慧</h3>
<p>小米小爱同学端侧AI负责人杨永杰在近期的一次访谈中指出，尽管端侧大模型被广泛视为未来的重要方向，但其商业化落地进程相对缓慢，核心症结在于端侧设备固有的<strong>资源限制</strong>和大模型<strong>快速迭代</strong>的特性<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。云端大模型可以轻松承载数十甚至数百亿参数，且能快速更新。但端侧设备仅能支持参数量约40亿（4B）的模型，且即使经过低比特量化，也可能牺牲模型效果。此外，端侧模型更新机制滞后，难以跟上云端日新月异的迭代速度。杨永杰认为，目前的端侧大模型更像是“技术积累”，为未来硬件和模型成熟做准备<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>面对这些挑战，小米团队选择了一条更为艰难但可能更具前景的道路：<strong>自研大模型推理框架</strong>。市场上针对端侧大模型的开源框架稀缺，尤其在NPU（神经网络处理器）上，由于芯片厂商通常不开放接口，导致NPU推理框架往往只能由厂商自行开发。这使得云端框架如vLLM或SGLang的丰富优化手段难以直接移植到端侧。小米的全栈自研，正是为了实现对每一个模块的细致性能优化，并能借鉴云端经验进行深度适配<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>他们的努力取得了显著成果：实现了<strong>180 tokens/s</strong>的实时推理性能<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。这一速度在资源受限的端侧设备上堪称卓越，其背后凝结着多项系统级与模型级优化策略：</p>
<ul>
<li><strong>动态输入与动态上下文支持</strong>：传统NPU常采用静态图，要求固定输入尺寸，导致资源浪费。小米的框架通过自动切分输入尺寸，让模型在保持静态图性能的同时支持动态输入，大幅提升了资源利用率和吞吐率。</li>
<li><strong>投机推理（Speculative Decoding）优化</strong>：这项技术在云端通常能带来2-3倍的加速，而小米通过自研策略在端侧实现了高达<strong>7-10倍的解码加速</strong>。这意味着原本可能只有20 tokens/s的速度，能够提升到足以媲美部分云端场景的200 tokens/s。</li>
<li><strong>量化与指令级优化</strong>：通过对模型进行低比特量化，以及利用CPU的Neon指令集对量化、反量化、采样等关键操作进行加速，进一步榨取硬件性能。</li>
</ul>
<p>这些工程实践不仅验证了端侧大模型的高性能潜力，也为车载等多模态应用场景的落地奠定了基础<sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<h3 id="架构创新多任务并发与资源复用">架构创新：多任务并发与资源复用</h3>
<p>小爱同学作为一款语音助手，需要承载语音控制、多轮对话、智能家居等多种任务。虽然目前单个请求链路通常是串行的，但模型能力会被其他业务共用，导致业务间可能出现并发冲突。更深层次的挑战在于，端侧设备的NPU通常<strong>不支持并发推理</strong>，其硬件设计以串行执行为主。试图通过_multi-batch_提升并发效率在端侧也收效甚微，因为单条请求本身就可能已接近设备算力的上限<sup id="fnref4:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>为了在有限的存储空间和内存下支持多任务、多业务场景，小米团队采用了**“共享基座模型 + 插件化能力”**的创新架构。例如，一台12GB内存的手机部署一个4B大模型可能就需要3GB内存，实际可分配给大模型的空间可能更少。在这种背景下，为每个业务部署独立模型是不可行的。</p>
<p>小米的解决方案是：选择一个统一的基础大模型作为“基座”，然后针对不同的业务单独训练轻量级的<strong>LoRA（Low-Rank Adaptation）模块</strong>。当A业务的请求到来时，加载基座模型与A的LoRA进行推理；B业务请求到来时，则卸载A的LoRA，换上B的LoRA。这种机制在只保留一个基础模型的前提下，通过LoRA的插件化和轻量化特性，实现了在资源有限的设备上<strong>动态切换不同业务能力</strong>，从而支持多个任务的并发调用<sup id="fnref5:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。这种“参数共享 + 差异定制”的模式，在内存利用率和扩展能力上都显示出显著优势。</p>
<p>在跨芯片平台部署方面，小米的推理框架采用了模块化、后端解耦的设计，与传统模型框架处理多后端问题异曲同工。杨永杰指出，大模型虽然规模庞大，但在框架设计层面，与底层硬件的绑定程度反而没有传统模型那么深，使得抽象出通用接口、实现跨平台迁移变得相对容易<sup id="fnref6:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>性能优化策略的组合与优先级也是工程实践中的关键。小米团队倾向于优先实现那些技术价值大、适用面广、且与其他手段不冲突的优化方式，例如<strong>并行解码、低比特量化、带宽控制</strong>等都可以组合使用。对于并非所有业务都需要的优化（如_prompt cache_），则将其封装为业务可选的配置项，降低使用复杂度，同时提供针对性的性能提升建议<sup id="fnref7:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<h3 id="展望未来算力与模型架构的双轮驱动">展望未来：算力与模型架构的双轮驱动</h3>
<p>在杨永杰看来，端侧大模型未来最具突破性的方向将集中在两大核心目标：</p>
<ol>
<li><strong>面向大模型优化的硬件能力提升</strong>：当前端侧大模型的诸多限制，如功耗高、挤占其他业务资源导致系统卡顿，都直接源于硬件算力不足。如同当年传统AI模型兴起催生了NPU，杨永杰预计，随着大模型热度持续，新一代面向大模型的端侧芯片将应运而生。一旦这类硬件普及，端侧模型的能力将得到大幅增强，从而让更多业务真正落地，实现商业化<sup id="fnref8:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
<li><strong>模型架构的演进</strong>：目前主流大模型多基于Transformer架构，其自回归特性导致当上下文（context）变长时，资源占用（特别是KV cache）会显著增加，内存和算力压力陡增。业界正在探索的<strong>Linear Attention</strong>架构，如<strong>Mamba、RWKV</strong>等，尝试在保持模型能力的同时，使内存使用与输入长度无关。这项技术对于资源敏感的端侧场景至关重要，尤其是在多模态任务中，图片、视频等非文本输入会迅速拉长上下文，Transformer的瓶颈将愈发凸显。尽管Linear Attention目前整体效果尚不及Transformer，但其研究热度与潜力预示着未来实用级别突破的可能性<sup id="fnref9:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
</ol>
<p>小米小爱同学团队的实践，无疑为全球范围内的边缘AI部署提供了宝贵的经验和范例。这不仅仅是技术性能的简单提升，更是对AI普惠化未来的一次深度探索。当AI不再局限于云端，而是真正融入我们日常使用的每一个设备时，它将以更低的延迟、更高的隐私保护、更强大的离线能力，深刻改变我们与数字世界的互动方式。</p>
<h2 id="引文">引文</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://www.infoq.cn/article/ifffkybhkhafkxdi4iac" target="_blank">小爱同学在高性能端侧大模型推理的实践｜AICon北京</a>·InfoQ·罗燕珊（2025/6/24）·检索日期2025/6/24&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref9:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://www.163.com/dy/article/K2R51P7D0511D3QS.html" target="_blank">小米小爱同学：资源受限下，实现端侧大模型的高性能推理</a>·网易（2025/6/24）·检索日期2025/6/24&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/%E7%AB%AF%E4%BE%A7ai/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">端侧AI</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">大模型推理</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%B0%8F%E7%B1%B3%E5%B0%8F%E7%88%B1%E5%90%8C%E5%AD%A6/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">小米小爱同学</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E8%B5%84%E6%BA%90%E5%8F%97%E9%99%90/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">资源受限</a>
   </li>
  
   <li class="list di">
     <a href="/tags/lora/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">LoRA</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%8A%95%E6%9C%BA%E6%8E%A8%E7%90%86/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">投机推理</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%85%B1%E4%BA%AB%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">共享基座模型</a>
   </li>
  
   <li class="list di">
     <a href="/tags/linear-attention/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Linear Attention</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">边缘计算</a>
   </li>
  
   <li class="list di">
     <a href="/tags/aicon/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">AICon</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3" style="color: #666;">相关阅读</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/insights/muai-20250624151004533-3/">微软Mu：操作系统中的微型AI革命，重塑本地智能交互边界</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250624101004329-0/">AI手机核心之争：芯片巨头如何在性能、架构与生态中角逐未来</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250623181004317-3/">更深层次的变革：AI如何重塑软件开发的起点、过程与目的</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250620161004296-1/">大模型幻觉之殇与协同之光：智能投顾如何精准破局</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250620111004334-2/">黑芝麻智能的战略棋局：低功耗芯片如何加速具身智能浪潮</a>
        </li>
	    
    </ul>
</div>



<div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links mt3">
  <p class="f5 b mb3" style="color: #666;">AI写作声明</p>
  <div class="f6 lh-copy" style="color: #777;">
    <p>本文由AI系统全流程生成，涵盖选题、资料整合与撰写。文章发布后由人工编辑进行核查。文末附有详细的[引用信息索引]，供您查证溯源。</p>
  </div>
</div></aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1315/" >
    &copy;  AI内参 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

    <script src="/js/relative-time.js"></script>
  </body>
</html>
