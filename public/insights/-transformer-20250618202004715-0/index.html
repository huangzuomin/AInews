<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>超越 Transformer：具身智能能否摆脱“水土不服”的困境？ | AI内参</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="当前，Transformer大模型在具身智能领域面临“水土不服”的挑战，主要原因在于硬件不稳定、数据稀缺以及大模型架构在能耗、泛化能力和物理世界理解上的局限。专家指出，具身智能正从模块化向端到端架构演进，并呼吁超越现有Transformer范式，探索能耗更低、更适应物理世界的新型模型架构，以实现“具身”与“智能”的真正融合。">
    <meta name="generator" content="Hugo 0.147.8">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    <meta name="author" content="温故智新AIGC实验室">
    

    


<link rel="stylesheet" href="/ananke/css/main.min.css" >



<link rel="stylesheet" href="/css/social-share.css">



  
    <link rel="stylesheet" href="/css/custom.css">
  

  
    <link rel="stylesheet" href="/css/article-enhancements.css">
  

    

<script src="/js/social-share.js"></script>



    
      

    

    

    
      <link rel="canonical" href="http://192.168.50.247:1313/insights/-transformer-20250618202004715-0/">
    

    <meta property="og:url" content="http://192.168.50.247:1313/insights/-transformer-20250618202004715-0/">
  <meta property="og:site_name" content="AI内参">
  <meta property="og:title" content="超越 Transformer：具身智能能否摆脱“水土不服”的困境？">
  <meta property="og:description" content="当前，Transformer大模型在具身智能领域面临“水土不服”的挑战，主要原因在于硬件不稳定、数据稀缺以及大模型架构在能耗、泛化能力和物理世界理解上的局限。专家指出，具身智能正从模块化向端到端架构演进，并呼吁超越现有Transformer范式，探索能耗更低、更适应物理世界的新型模型架构，以实现“具身”与“智能”的真正融合。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="insights">
    <meta property="article:published_time" content="2025-06-18T20:20:04+08:00">
    <meta property="article:modified_time" content="2025-06-18T20:20:04+08:00">
    <meta property="article:tag" content="具身智能">
    <meta property="article:tag" content="Transformer">
    <meta property="article:tag" content="大模型">
    <meta property="article:tag" content="机器人">
    <meta property="article:tag" content="人工智能">
    <meta property="article:tag" content="硬件瓶颈">

  <meta itemprop="name" content="超越 Transformer：具身智能能否摆脱“水土不服”的困境？">
  <meta itemprop="description" content="当前，Transformer大模型在具身智能领域面临“水土不服”的挑战，主要原因在于硬件不稳定、数据稀缺以及大模型架构在能耗、泛化能力和物理世界理解上的局限。专家指出，具身智能正从模块化向端到端架构演进，并呼吁超越现有Transformer范式，探索能耗更低、更适应物理世界的新型模型架构，以实现“具身”与“智能”的真正融合。">
  <meta itemprop="datePublished" content="2025-06-18T20:20:04+08:00">
  <meta itemprop="dateModified" content="2025-06-18T20:20:04+08:00">
  <meta itemprop="wordCount" content="31">
  <meta itemprop="keywords" content="具身智能,Transformer,大模型,机器人,人工智能,硬件瓶颈,数据稀缺,架构创新">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="超越 Transformer：具身智能能否摆脱“水土不服”的困境？">
  <meta name="twitter:description" content="当前，Transformer大模型在具身智能领域面临“水土不服”的挑战，主要原因在于硬件不稳定、数据稀缺以及大模型架构在能耗、泛化能力和物理世界理解上的局限。专家指出，具身智能正从模块化向端到端架构演进，并呼吁超越现有Transformer范式，探索能耗更低、更适应物理世界的新型模型架构，以实现“具身”与“智能”的真正融合。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
  
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://192.168.50.247:1313/images/default%20%283%29.png');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        <img src="/logo/logo.png" class="w100 mw5-ns" alt="AI内参" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="">
              首页
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/insights/" title="">
              洞察
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/newspaper/" title="">
              日报
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/explore/" title="">
              主题探索
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="">
              关于
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref helvetica tracked">
        <span style="color: #999;">2025-06-18 20:20</span>
      </aside><style>
   
  .ananke-socials {
    display: flex;
    align-items: center;
    gap: 10px;  
  }
  .ananke-social-link {
    display: inline-block;
    width: 32px;
    height: 32px;
  }
  .ananke-social-link .icon {
    display: block;
    width: 100%;
    height: 100%;
  }
  .ananke-social-link svg {
    width: 100%;
    height: 100%;
  }

   
  .wechat-qr-modal {
      display: none;
      position: fixed;
      z-index: 1000;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0,0,0,0.5);
      justify-content: center;
      align-items: center;
  }
  .wechat-qr-modal-content {
      background-color: white;
      padding: 20px;
      border-radius: 8px;
      text-align: center;
      position: relative;
      max-width: 300px;
      width: 90%;
      color: #333;
  }
  .wechat-qr-modal-close {
      position: absolute;
      top: 10px;
      right: 15px;
      font-size: 24px;
      font-weight: bold;
      cursor: pointer;
      color: #999;
  }
  .wechat-qr-modal-close:hover {
      color: #333;
  }
  #wechat-qr-code {
      max-width: 200px;
      height: auto;
      margin-top: 10px;
  }
  </style>

  <div id="sharing" class="mt3 ananke-socials"><a href="https://twitter.com/intent/tweet/?text=%E8%B6%85%E8%B6%8A&amp;#43;Transformer%EF%BC%9A%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%83%BD%E5%90%A6%E6%91%86%E8%84%B1%E2%80%9C%E6%B0%B4%E5%9C%9F%E4%B8%8D%E6%9C%8D%E2%80%9D%E7%9A%84%E5%9B%B0%E5%A2%83%EF%BC%9F%20-%20%E5%BD%93%E5%89%8D%EF%BC%8CTransformer%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E9%A2%86%E5%9F%9F%E9%9D%A2%E4%B8%B4%E2%80%9C%E6%B0%B4%E5%9C%9F%E4%B8%8D%E6%9C%8D%E2%80%9D%E7%9A%84%E6%8C%91%E6%88%98%EF%BC%8C%E4%B8%BB%E8%A6%81%E5%8E%9F%E5%9B%A0%E5%9C%A8%E4%BA%8E%E7%A1%AC%E4%BB%B6%E4%B8%8D%E7%A8%B3%E5%AE%9A%E3%80%81%E6%95%B0%E6%8D%AE%E7%A8%80%E7%BC%BA%E4%BB%A5%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E5%9C%A8%E8%83%BD%E8%80%97%E3%80%81%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E5%92%8C%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E7%90%86%E8%A7%A3%E4%B8%8A%E7%9A%84%E5%B1%80%E9%99%90%E3%80%82%E4%B8%93%E5%AE%B6%E6%8C%87%E5%87%BA%EF%BC%8C%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E6%AD%A3%E4%BB%8E%E6%A8%A1%E5%9D%97%E5%8C%96%E5%90%91%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%EF%BC%8C%E5%B9%B6%E5%91%BC%E5%90%81%E8%B6%85%E8%B6%8A%E7%8E%B0%E6%9C%89Transformer%E8%8C%83%E5%BC%8F%EF%BC%8C%E6%8E%A2%E7%B4%A2%E8%83%BD%E8%80%97%E6%9B%B4%E4%BD%8E%E3%80%81%E6%9B%B4%E9%80%82%E5%BA%94%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E7%9A%84%E6%96%B0%E5%9E%8B%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E2%80%9C%E5%85%B7%E8%BA%AB%E2%80%9D%E4%B8%8E%E2%80%9C%E6%99%BA%E8%83%BD%E2%80%9D%E7%9A%84%E7%9C%9F%E6%AD%A3%E8%9E%8D%E5%90%88%E3%80%82&amp;amp;url=http%3A%2F%2F192.168.50.247%3A1313%2Finsights%2F-transformer-20250618202004715-0%2F"
        class="ananke-social-link x-twitter no-underline"
        title="Share on X" aria-label="Share on X"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
              </span></a><a href="http://service.weibo.com/share/share.php?title=%E8%B6%85%E8%B6%8A&amp;#43;Transformer%EF%BC%9A%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E8%83%BD%E5%90%A6%E6%91%86%E8%84%B1%E2%80%9C%E6%B0%B4%E5%9C%9F%E4%B8%8D%E6%9C%8D%E2%80%9D%E7%9A%84%E5%9B%B0%E5%A2%83%EF%BC%9F%20-%20%E5%BD%93%E5%89%8D%EF%BC%8CTransformer%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E9%A2%86%E5%9F%9F%E9%9D%A2%E4%B8%B4%E2%80%9C%E6%B0%B4%E5%9C%9F%E4%B8%8D%E6%9C%8D%E2%80%9D%E7%9A%84%E6%8C%91%E6%88%98%EF%BC%8C%E4%B8%BB%E8%A6%81%E5%8E%9F%E5%9B%A0%E5%9C%A8%E4%BA%8E%E7%A1%AC%E4%BB%B6%E4%B8%8D%E7%A8%B3%E5%AE%9A%E3%80%81%E6%95%B0%E6%8D%AE%E7%A8%80%E7%BC%BA%E4%BB%A5%E5%8F%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E5%9C%A8%E8%83%BD%E8%80%97%E3%80%81%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E5%92%8C%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E7%90%86%E8%A7%A3%E4%B8%8A%E7%9A%84%E5%B1%80%E9%99%90%E3%80%82%E4%B8%93%E5%AE%B6%E6%8C%87%E5%87%BA%EF%BC%8C%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E6%AD%A3%E4%BB%8E%E6%A8%A1%E5%9D%97%E5%8C%96%E5%90%91%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%EF%BC%8C%E5%B9%B6%E5%91%BC%E5%90%81%E8%B6%85%E8%B6%8A%E7%8E%B0%E6%9C%89Transformer%E8%8C%83%E5%BC%8F%EF%BC%8C%E6%8E%A2%E7%B4%A2%E8%83%BD%E8%80%97%E6%9B%B4%E4%BD%8E%E3%80%81%E6%9B%B4%E9%80%82%E5%BA%94%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E7%9A%84%E6%96%B0%E5%9E%8B%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E2%80%9C%E5%85%B7%E8%BA%AB%E2%80%9D%E4%B8%8E%E2%80%9C%E6%99%BA%E8%83%BD%E2%80%9D%E7%9A%84%E7%9C%9F%E6%AD%A3%E8%9E%8D%E5%90%88%E3%80%82&amp;amp;url=http%3A%2F%2F192.168.50.247%3A1313%2Finsights%2F-transformer-20250618202004715-0%2F"
        class="ananke-social-link weibo no-underline"
        title="Share on Weibo" aria-label="Share on Weibo"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg>
              </span></a><a href="#ZgotmplZ"
        class="ananke-social-link weixin no-underline"
        title="Share on Weixin" aria-label="Share on Weixin"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154zm-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4zm-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2zM563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4zm-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6zm107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6z"/></svg>
              </span></a>
    <a href="javascript:void(0);" id="wechat-share-btn" class="ananke-social-link wechat no-underline" title="分享到微信" aria-label="分享到微信">
      <span class="icon">
        <svg viewBox="0 0 24 24" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path d="M12 2C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2zm4.5 14.5c-.28 0-.5-.22-.5-.5s.22-.5.5-.5h1c.28 0 .5.22.5.5s-.22.5-.5.5zm-3 0c-.28 0-.5-.22-.5-.5s.22-.5.5-.5h1c.28 0 .5.22.5.5s-.22.5-.5.5zm-9-4c-.28 0-.5-.22-.5-.5s.22-.5.5-.5h1c.28 0 .5.22.5.5s-.22.5-.5.5zm3 0c-.28 0-.5-.22-.5-.5s.22-.5.5-.5h1c.28 0 .5.22.5.5s-.22.5-.5.5zm-6.5 8.5c-2.53 0-4.61-1.92-4.94-4.36c-.04-.3.18-.57.48-.57c.25 0 .46.18.5.42c.28 1.88 1.85 3.31 3.82 3.31c.45 0 .88-.07 1.29-.2c.22-.07.46.02.58.22l.62.9c.15.22.1.5-.1.65c-.5.38-1.07.66-1.68.81c-.04.01-.07.02-.11.02m13.5-9.5c-3.63 0-6.68 2.56-7.33 5.89c-.06.28.15.54.43.54c.23 0 .43-.16.48-.39c.5-2.72 2.98-4.84 5.8-4.84c1.45 0 2.78.48 3.82 1.28c.18.14.45.1.57-.1l.73-.93c.16-.2.14-.5-.05-.66c-1.23-.98-2.8-1.59-4.55-1.59M6.11 20.11c.18.16.45.15.61-.02l.73-.8c.16-.17.14-.45-.04-.6c-.93-.8-1.5-1.9-1.5-3.09c0-2.48 2.02-4.5 4.5-4.5s4.5 2.02 4.5 4.5c0 .3-.02.6-.07.88c-.06.28.15.54.43.54c.24 0 .45-.17.5-.4c.06-.3.1-.6.1-.92c0-3.03-2.47-5.5-5.5-5.5s-5.5 2.47-5.5 5.5c0 1.48.58 2.82 1.54 3.82z"/></svg>
      </span>
    </a>
  </div>

  
  <div id="wechat-qr-modal" class="wechat-qr-modal">
      <div class="wechat-qr-modal-content">
          <span class="wechat-qr-modal-close" id="wechat-qr-modal-close">&times;</span>
          <p>请使用微信“扫一扫”分享</p>
          <p>在手机微信浏览器中，请点击右上角“...”菜单，选择“分享到朋友圈”或“发送给朋友”</p>
          <img id="wechat-qr-code" src="" alt="WeChat QR Code" />
      </div>
  </div>

  <script>
  document.addEventListener('DOMContentLoaded', function() {
      var wechatBtn = document.getElementById('wechat-share-btn');
      var modal = document.getElementById('wechat-qr-modal');
      var closeBtn = document.getElementById('wechat-qr-modal-close');
      var qrCodeImg = document.getElementById('wechat-qr-code');

      if (wechatBtn && modal && closeBtn && qrCodeImg) {
          wechatBtn.addEventListener('click', function(e) {
              e.preventDefault();
              var pageUrl = "http:\/\/192.168.50.247:1313\/insights\/-transformer-20250618202004715-0\/";
              qrCodeImg.src = "https://api.qrserver.com/v1/create-qr-code/?size=200x200&data=" + encodeURIComponent(pageUrl);
              modal.style.display = 'flex';
          });

          closeBtn.addEventListener('click', function() {
              modal.style.display = 'none';
          });

          window.addEventListener('click', function(event) {
              if (event.target == modal) {
                  modal.style.display = 'none';
              }
          });
      }
  });
  </script><h1 class="f1 athelas mt3 mb1">超越 Transformer：具身智能能否摆脱“水土不服”的困境？</h1>
      
      <p class="tracked"><strong>温故智新AIGC实验室</strong>
      </p>
      
      
    </header>
    <div class="article-content nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><blockquote>
<p>尽管Transformer大模型在AI领域屡创奇迹，但在具身智能的物理世界中，它们正遭遇“水土不服”的挑战。核心症结在于现有架构的能耗、泛化能力局限以及对真实物理世界的理解不足，促使研究者探索超越传统范式的软硬件协同进化之路。</p></blockquote>
<p>在人工智能领域，2025年被业界寄予厚望，被称为“具身智能元年”并非偶然。从宇树机器人登上春晚，到科技巨头和明星创业者纷纷入局，通用机器人时代仿佛触手可及。然而，在这股热潮之下，质疑的声浪也从未平息。英伟达CEO黄仁勋宣称“通用机器人时代已经到来”，而知名投资人朱啸虎却宣布“正批量退出人形机器人公司”<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，这鲜明的对比揭示了具身智能领域技术理想与工程现实之间的巨大鸿沟。</p>
<p>CSDN《万有引力》栏目近期邀请到同济大学计算机科学与技术学院教授胡亮和智源具身智能大模型负责人王鹏伟，两位深耕AI与机器人领域的专家深入探讨了当前具身智能所面临的关键挑战：究竟是模型不够强大，数据量不足，还是底层架构存在根本性缺陷？</p>
<h3 id="从虚拟到现实具身智能的阵痛期">从虚拟到现实：具身智能的“阵痛期”</h3>
<p>从多模态大模型走向具身智能，看似是一次“丝滑”的技术跃迁，然而，王鹏伟坦言，团队在实际落地过程中经历了明显的“阵痛期”<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。尽管大语言模型在语言生成和推理能力上表现出色，一旦部署到真实的物理环境中，便频频遭遇挫折。核心问题在于<strong>硬件的不稳定性</strong>和<strong>真实世界数据的匮乏</strong>。机器人硬件故障频发导致维护成本高昂，直接阻碍了系统大规模部署和数据采集。这种“硬件不稳定导致难以落地，难以落地又造成数据不足”的恶性循环，成为了具身智能发展的主要瓶颈。</p>
<p>当前的具身智能领域，主要存在两种技术路线：<strong>模块化的流水线架构（Pipeline）<strong>和</strong>端到端架构</strong>。王鹏伟指出，这两种范式在AI应用的历史中屡见不鲜，例如在ChatGPT出现之前，搜索和聊天系统多采用模块化设计，指令经过命名实体识别、语法解析等多个独立模块处理。而ChatGPT则通过一个端到端的大模型统一处理所有任务，展现出更高的性能上限和对长尾问题的处理能力<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>这种从Pipeline到端到端的演进，其背后驱动力正是<strong>数据量的激增和底层算力结构的升级</strong>，尤其是Transformer架构的出现，使得大规模基础模型的训练成为可能。王鹏伟强调，无论是大模型、自动驾驶还是具身智能，其技术发展路径基本高度一致，都遵循从模块化向端到端的过渡。当前，智驾领域“端到端”已成为衡量技术先进性的重要标签。智源研究院发布的RoboOS和RoboBrain框架，正是为了弥合这一差距，提供统一的接口范式，兼容主流机器人设备，帮助用户快速部署，进而积累数据以推动系统向更强大的端到端架构演进。</p>
<p>此外，具身智能领域还存在一个本土化概念——<strong>“大小脑”</strong>，它与西方技术圈的“快系统”（System 1）和“慢系统”（System 2）异曲同工<sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。快系统（小脑）负责高频率的实时控制，如抓取、放置等动作执行，追求丝滑流畅的响应速度（30Hz以上）；慢系统（大脑）则处理逻辑推理和复杂感知，参数量大，响应频率相对较低（如7B模型上限10Hz）。这种双系统架构旨在平衡实时响应与复杂推理的需求，以适应物理世界的多变性。</p>
<h3 id="transformer架构的深层瓶颈">Transformer架构的深层瓶颈</h3>
<p>尽管Transformer架构凭借其并行处理能力和捕捉长距离依赖的优势，成为当前AI大模型的主流<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，但在具身智能的背景下，胡亮教授提出其存在<strong>深层次的“水土不服”</strong><sup id="fnref4:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>首先是<strong>能耗问题</strong>。当前大模型动辄千亿参数，需要数万块GPU支撑，训练和运行成本极高，功耗巨大<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>。将这样一个“高能耗大脑”植入机器人，显然不现实。人类大脑的功耗远低于GPU集群，却能处理更复杂的任务，这暗示现有大模型架构仍有巨大改进空间。</p>
<p>其次是<strong>泛化能力和任务适应性</strong>的局限。目前的AI算法在应对任务差异性较大的情境时表现乏力，导致“一类任务一台机器人”的碎片化现状，与人类的通用智能相去甚远。大模型在遇到新任务时，其快速适应和学习能力仍较差，频繁的再训练成本也令人望而却步<sup id="fnref5:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>更核心的挑战在于<strong>Transformer架构本身的特性</strong>。它是一种“存算一体”的模式，知识和任务执行逻辑紧密耦合在庞大的参数中。参数越多，知识越丰富，模型能力越强，但也越脆弱。当模型学习新知识时，可能意外破坏已有任务执行能力，导致机器人协作不稳定。这与人脑中知识和本能任务执行相对分离的机制截然不同。基于此，胡亮团队认为，当前以Transformer为基础的大模型架构可能并不适用于具身智能的长期发展目标，需要从架构层面进行优化，例如借鉴早期的**记忆网络（Memory Networks）<strong>和</strong>神经图灵机（Neural Turing Machine）**思想，<strong>将记忆与逻辑执行分离开来</strong><sup id="fnref6:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。这种模块化设计能让模型在执行特定任务时仅激活相关模块，从而降低计算消耗，提高效率，并允许在不影响核心执行能力的情况下，通过扩展记忆模块来学习新知识，甚至实现多智能体协作。</p>
<p>此外，<strong>感知层面</strong>的“Gap”也异常显著。机器人配备多种传感器（触觉、语音、视觉），而当前大模型主要基于自然语言或图像训练，与物理世界的多模态传感器体系不匹配<sup id="fnref7:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。大模型在处理图像时，更多是对象识别，而非真正理解图像内涵和物理常识。例如，模型可能生成“马骑人”的图片，因为其对“上下左右”等空间关系仅停留在语义向量层面，无法理解其在物理空间中的真实指向性<sup id="fnref8:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。这种将物理场景转化为文字描述再交由大语言模型学习的方式，会简化甚至丢失关键的空间物理关系，是实现具备真实环境感知能力大模型的长路上的巨大障碍。胡亮团队正探索将真实世界的空间关系直接引入模型训练，甚至尝试<strong>直接提取人脑中的意图信号并解码为AI可理解信息，实现“人脑—AI大脑”的直连</strong>，以此绕过语言等中间步骤，大幅提升人机交互和协作效率，以应对自动驾驶等高风险场景下的毫秒级决策需求<sup id="fnref9:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<h3 id="迈向真正具身与智能的融合">迈向真正“具身”与“智能”的融合</h3>
<p>黄仁勋宣称“通用机器人时代已经到来”，但从具身智能研究者的角度看，这仍是一个遥远的目标。现实中的“模拟器到真实环境的Gap”依然巨大，因为仿真环境难以穷举现实中的所有物理属性<sup id="fnref10:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。即使模拟做得再好，真实世界的摩擦力、材质硬度等微小差异都可能导致模型在实际部署中失效。</p>
<p>因此，专家们普遍认为，未来五到十年，将是<strong>软硬件交替迭代的关键期</strong><sup id="fnref11:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。算法层面的升级会推动硬件架构的进步，而硬件的进步又会反过来加速算法研究的演进。例如，人形机器人由于其高重心和双足结构，在导航等基础任务上对传感器系统造成更大挑战，需要硬件本体和控制算法的同步优化。</p>
<p>尽管面临重重挑战，具身智能的发展趋势依然清晰：从目前偏向“专用型”的机器人，正逐步向<strong>多任务混合训练</strong>发展。只要机器人具备一个强大的基础模型，便有望根据指令激活模型中的不同区域，从而执行多种不同任务，实现从“我只会这一个任务”到“我可以完成多种任务”的转变<sup id="fnref12:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>正如王鹏伟所言，具身智能是一条“正确但非常艰难的道路”<sup id="fnref13:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。它不仅需要对Transformer等现有大模型架构进行颠覆性创新，以实现低能耗、强泛化和对物理世界的深刻理解，更需要软硬件的深度协同与进化。只有当“具身”与“智能”真正协作起来，弥合感知、理解、决策与行动之间的鸿沟，我们才能期待一个能像人类一样在家中完成各项任务的通用机器人真正到来。</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>CSDN（近期）。<a href="https://mp.weixin.qq.com/s?__biz=MzkzMDY1NDgyOQ==&amp;mid=2247819234&amp;idx=1&amp;sn=863b40504a477a4fbc10ec97514e1edf&amp;chksm=c32018ee9a4d21f7b8c90eb95524372ac123dfe13d1432d5630d0e2e7f4f085f77eca91fc6d5&amp;scene=0&amp;xtrack=1#rd">Transformer 在具身智能“水土不服”，大模型强≠机器人强</a>。CSDN。检索日期2024/06/19。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref8:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref9:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref10:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref11:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref12:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref13:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>51CTO博客（2023/12/28）。<a href="https://blog.51cto.com/u_15620990/12852483">大模型 | 一文彻底搞懂深度学习：Transformer</a>。51CTO博客。检索日期2024/06/19。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>未来智库（2023/12/26）。<a href="https://www.vzkoo.com/read/20250120982a8eb84de49552c13277a7.html">2025年AI大模型专题报告：Transformer架构的过去、现在和未来</a>。未来智库。检索日期2024/06/19。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">具身智能</a>
   </li>
  
   <li class="list di">
     <a href="/tags/transformer/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Transformer</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">大模型</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">机器人</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">人工智能</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E7%A1%AC%E4%BB%B6%E7%93%B6%E9%A2%88/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">硬件瓶颈</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%95%B0%E6%8D%AE%E7%A8%80%E7%BC%BA/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">数据稀缺</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">架构创新</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3" style="color: #666;">相关阅读</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/insights/article-20250618202004724-1/">破解AI心智之谜：深入探究其推理机制、幻觉与欺骗的深层逻辑</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/llmmit50-20250618172004590-1/">信息洪流中的LLM深度航标：MIT揭示掌握大模型精髓的50个关键洞察</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/deepmindai-20250618142004277-0/">突破“垃圾进，垃圾出”魔咒：谷歌DeepMind如何用元学习重塑AI数据筛选</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/gemini-25ai-20250618122004575-1/">谷歌Gemini 2.5系列模型稳定发布：性能、性价比与AI生态的深层博弈</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/gemini-25aiopenai-20250618062004410-0/">谷歌Gemini 2.5：以“思考”模型重塑企业AI赛道，剑指OpenAI主导地位</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250617202000381-5/">AI眼镜：从“百镜大战”到下一代计算平台的漫漫长路</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/graai-20250617202000362-3/">集体智能的崛起：GRA框架如何赋能小模型“逆袭”大模型，重塑AI开发图景</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250617193006131-3/">巨头AI的“开源”之梦：广告驱动下的商业化新纪元</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250617190043078-5/">巨头AI的“开源”之梦：广告驱动下的商业化新纪元</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/article-20250617083004626-3/">中国大模型“六小虎”高管震荡：从“狂飙突进”到“断臂求生”的行业转折</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/aio3-pro-20250617025225363-5/">当“推箱子”邂逅AI：o3-pro在经典游戏基准测试中突破上限</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/ipo-20250616123004/">具身智能浪潮下的港股叩门者：乐动机器人IPO揭示的视觉感知技术与市场竞逐</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/555/">智源系新星崛起：智在无界如何以“人类行为数据”重塑人形机器人大脑</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/dddf/">人形机器人大模型新篇章：智在无界如何突破具身智能瓶颈</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/insights/salesforceinformaticaai-20250618202004751-4/">Salesforce收购Informatica：AI时代数据治理的战略转向</a>
        </li>
	    
    </ul>
</div>



<div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links mt3">
  <p class="f5 b mb3" style="color: #666;">AI写作声明</p>
  <div class="f6 lh-copy" style="color: #777;">
    <p>本文由AI系统全流程生成，涵盖选题、资料整合与撰写。文章发布后由人工编辑进行核查。文末附有详细的[引用信息索引]，供您查证溯源。</p>
  </div>
</div></aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://192.168.50.247:1313/" >
    &copy;  AI内参 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

    <script src="/js/relative-time.js"></script>
  </body>
</html>
