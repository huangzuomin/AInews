<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI伦理与治理 on AI内参</title>
    <link>http://192.168.50.247:1313/main_topics/ai%E4%BC%A6%E7%90%86%E4%B8%8E%E6%B2%BB%E7%90%86/</link>
    <description>Recent content in AI伦理与治理 on AI内参</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 21 Jun 2025 18:10:04 +0800</lastBuildDate>
    <atom:link href="http://192.168.50.247:1313/main_topics/ai%E4%BC%A6%E7%90%86%E4%B8%8E%E6%B2%BB%E7%90%86/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大型语言模型的幻象：苹果争议揭示通用智能之路的挑战</title>
      <link>http://192.168.50.247:1313/articles/article-20250621181004290-0/</link>
      <pubDate>Sat, 21 Jun 2025 18:10:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/article-20250621181004290-0/</guid>
      <description>苹果公司一篇质疑大型语言模型（LLM）推理能力和存在“准确率崩溃”的论文，在AI社区引发了激烈辩论，挑战了“规模化即一切”的行业信念。尽管面临来自AI专家和AI模型Claude本身的驳斥，但纽约大学教授加里·马库斯反驳了这些质疑，并获得了Salesforce和UC伯克利研究的间接支持，这些研究揭示了LLM在多轮推理和视觉理解上的脆弱性与隐私问题，促使业界重新思考AI的评估范式和神经符号结合等未来架构方向。</description>
    </item>
    <item>
      <title>Mistral Small 3.2：高效能模型的战略升级与欧洲AI主权的崛起</title>
      <link>http://192.168.50.247:1313/articles/mistral-small-32ai-20250621071004217-0/</link>
      <pubDate>Sat, 21 Jun 2025 07:10:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/mistral-small-32ai-20250621071004217-0/</guid>
      <description>法国AI初创公司Mistral AI将其开源小型模型Mistral Small从3.1升级至3.2，此次迭代着重于提升性能和效率，而非扩大参数规模，展现了其在“小而精”模型路线上的坚持。凭借240亿参数即可媲美大型模型的强大能力，以及对欧盟AI法规的严格遵循，Mistral不仅在开放模型市场占据优势，更在全球AI主权竞争中扮演着关键角色，为企业提供了高效且合规的AI解决方案。</description>
    </item>
    <item>
      <title>揭示权力与利润的交织：OpenAI深陷信任危机</title>
      <link>http://192.168.50.247:1313/articles/openai-20250620211005699-4/</link>
      <pubDate>Fri, 20 Jun 2025 21:10:05 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/openai-20250620211005699-4/</guid>
      <description>一份名为《OpenAI档案》的深度报告揭露了OpenAI从非营利研究机构向营利巨头的转变，并详细披露了CEO奥特曼在公司治理、安全承诺和个人利益冲突方面的诸多不当行为。报告质疑OpenAI背弃其“为人类谋福祉”的创立使命，将利润和增长置于安全与透明之上，这引发了对AI行业伦理、监管和未来发展方向的深刻担忧。</description>
    </item>
    <item>
      <title>当AI成为“外部大脑”：MIT研究揭示ChatGPT对人类认知的深层影响与“认知惯性”</title>
      <link>http://192.168.50.247:1313/articles/aimitchatgpt-20250620201004425-1/</link>
      <pubDate>Fri, 20 Jun 2025 20:10:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/aimitchatgpt-20250620201004425-1/</guid>
      <description>麻省理工学院一项最新研究指出，过度使用ChatGPT等大型语言模型可能导致大脑活动水平下降，削弱记忆并引发“认知惯性”。这项结合脑电图与自然语言处理的实验发现，长期依赖AI会使大脑从主动生成信息转变为被动筛选信息，影响深度思考和创造力，提示人类需警惕AI对认知能力的潜在负面影响，并在工具使用与自主思考间寻求平衡。</description>
    </item>
    <item>
      <title>揭秘Gemini透明度迷雾：谷歌的“黑箱”决策如何挑战开发者信任与AI伦理</title>
      <link>http://192.168.50.247:1313/articles/geminiai-20250620201004432-2/</link>
      <pubDate>Fri, 20 Jun 2025 20:10:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/geminiai-20250620201004432-2/</guid>
      <description>谷歌近期削减Gemini模型推理过程透明度的决定，引发了开发者社区的强烈不满，许多企业用户因无法有效调试而感到“盲目”。这一举动不仅损害了开发者对谷歌AI平台的信任，也凸显了前沿AI模型在性能与可解释性之间的内在矛盾，并对AI伦理、问责制以及谷歌在激烈AI竞赛中的市场地位构成了深远挑战。</description>
    </item>
    <item>
      <title>人造人类：在共进化与共存的十字路口重新定义人类</title>
      <link>http://192.168.50.247:1313/articles/article-20250619182004369-0/</link>
      <pubDate>Thu, 19 Jun 2025 18:20:04 +0800</pubDate>
      <guid>http://192.168.50.247:1313/articles/article-20250619182004369-0/</guid>
      <description>随着人工智能的飞速发展，“人造人类”的可能性正迫使我们重新审视人类的定义和未来。文章深入探讨了人类与AI“共同进化”与“共存”的两种路径及其潜在风险，强调了通过将人类价值观和“共识”编码入AI，以及重新定义人类尊严的重要性，以期在AI时代维系人类的自主性与核心价值。</description>
    </item>
  </channel>
</rss>
