<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>超越表象：大语言模型“遗忘”的深层结构与可逆边界 | AI内参</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="一项由香港理工大学、卡内基梅隆大学和加州大学圣克鲁兹分校共同完成的开创性研究，首次系统揭示了大语言模型“遗忘”现象背后的深层表示结构变化。研究区分了“可逆性遗忘”与“不可逆性遗忘”的本质差异，强调真正的遗忘是结构性抹除而非行为抑制，并通过一套表示空间诊断工具，为构建更安全、可控的机器遗忘机制奠定了基础。">
    <meta name="generator" content="Hugo 0.147.0">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    <meta name="author" content="AI内参团队">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  
    <link rel="stylesheet" href="/css/custom.css">
  


    


    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/articles/article-20250616123004/">
    

    <meta property="og:url" content="http://localhost:1313/articles/article-20250616123004/">
  <meta property="og:site_name" content="AI内参">
  <meta property="og:title" content="超越表象：大语言模型“遗忘”的深层结构与可逆边界">
  <meta property="og:description" content="一项由香港理工大学、卡内基梅隆大学和加州大学圣克鲁兹分校共同完成的开创性研究，首次系统揭示了大语言模型“遗忘”现象背后的深层表示结构变化。研究区分了“可逆性遗忘”与“不可逆性遗忘”的本质差异，强调真正的遗忘是结构性抹除而非行为抑制，并通过一套表示空间诊断工具，为构建更安全、可控的机器遗忘机制奠定了基础。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2025-06-16T12:30:04+08:00">
    <meta property="article:modified_time" content="2025-06-16T12:30:04+08:00">
    <meta property="article:tag" content="大语言模型">
    <meta property="article:tag" content="机器遗忘">
    <meta property="article:tag" content="AI伦理">
    <meta property="article:tag" content="数据隐私">
    <meta property="article:tag" content="表示学习">
    <meta property="article:tag" content="人工智能安全">

  <meta itemprop="name" content="超越表象：大语言模型“遗忘”的深层结构与可逆边界">
  <meta itemprop="description" content="一项由香港理工大学、卡内基梅隆大学和加州大学圣克鲁兹分校共同完成的开创性研究，首次系统揭示了大语言模型“遗忘”现象背后的深层表示结构变化。研究区分了“可逆性遗忘”与“不可逆性遗忘”的本质差异，强调真正的遗忘是结构性抹除而非行为抑制，并通过一套表示空间诊断工具，为构建更安全、可控的机器遗忘机制奠定了基础。">
  <meta itemprop="datePublished" content="2025-06-16T12:30:04+08:00">
  <meta itemprop="dateModified" content="2025-06-16T12:30:04+08:00">
  <meta itemprop="wordCount" content="32">
  <meta itemprop="keywords" content="大语言模型,机器遗忘,AI伦理,数据隐私,表示学习,人工智能安全,可逆遗忘,结构性抹除">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="超越表象：大语言模型“遗忘”的深层结构与可逆边界">
  <meta name="twitter:description" content="一项由香港理工大学、卡内基梅隆大学和加州大学圣克鲁兹分校共同完成的开创性研究，首次系统揭示了大语言模型“遗忘”现象背后的深层表示结构变化。研究区分了“可逆性遗忘”与“不可逆性遗忘”的本质差异，强调真正的遗忘是结构性抹除而非行为抑制，并通过一套表示空间诊断工具，为构建更安全、可控的机器遗忘机制奠定了基础。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
  
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        AI内参
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="">
              首页
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/articles/" title="">
              新闻
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/categories/" title="">
              分类
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/tags/" title="">
              标签
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/authors/" title="">
              作者
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="">
              关于
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Articles
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1" style="display: none;">超越表象：大语言模型“遗忘”的深层结构与可逆边界</h1>
      
      <p class="tracked"><strong>AI内参团队</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-06-16T12:30:04+08:00">June 16, 2025</time>
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><blockquote>
<p>大语言模型的“遗忘”并非简单的信息删除，而是其内部表示结构发生变化的体现。一项开创性研究首次系统性地区分了“可逆性遗忘”与“不可逆性遗忘”，揭示了真正的遗忘在于结构性抹除，而非仅仅是行为上的抑制，这为未来构建更安全、更可控的AI模型提供了关键工具与深刻洞察。</p></blockquote>
<p>在人工智能能力不断突破边界的当下，大语言模型（LLMs）所带来的隐私与数据治理挑战日益凸显。当模型“记住”了训练数据中的敏感信息，并可能在推理时无意中暴露，如何让这些强大的系统有选择地“忘记”特定知识，成为了一个迫切的、横亘在技术前沿与伦理边界之间的难题。**机器遗忘（Machine Unlearning）**技术应运而生，旨在实现高效且精确的信息抹除，同时不损及模型的整体能力。然而，一个长久以来被忽视的深刻问题是：当前依赖于token级别表现（如准确率、困惑度）的评估方法，真的足以证明模型已经“遗忘”了吗？</p>
<p>最近，由香港理工大学、卡内基梅隆大学和加州大学圣克鲁兹分校的研究人员组成的团队，在这一领域取得了突破性进展。他们首次深入剖析了LLM遗忘现象背后更为根本的表示结构变化规律，并构建了一套强大的表示空间诊断工具。这项研究不仅区分了**“可逆性遗忘”与“灾难性不可逆遗忘”的本质差异**，更揭示了模型“遗忘”的真正核心在于其内部结构的重塑，而非仅仅是表层行为的抑制<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。</p>
<h3 id="深入理解机器遗忘表象与本质">深入理解“机器遗忘”：表象与本质</h3>
<p>长期以来，对机器遗忘的理解往往停留在模型输出行为的变化上——例如，模型不再能准确回答某个特定问题，或者不再生成包含敏感信息的文本。但这项研究的核心洞察挑战了这种表层观念。研究人员明确指出：“一个模型若仅仅在token输出上‘忘记’，而其内部结构几乎未变，那它随时可以恢复原样。”<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>这意味着，传统的“遗忘”可能只是行为上的抑制，而非真正的知识清除。例如，在某些情况下，尽管模型在遗忘特定数据后表现出准确率急剧下降，但一旦通过“重学习”（Relearning）机制，其性能又能迅速恢复，这表明模型内部的知识表征并未被彻底抹除，而只是被暂时“抑制”或“隐藏”起来。研究团队将此定义为**“可逆性遗忘”**。</p>
<p>与之相对的是**“不可逆性遗忘”**。在这种场景下，模型不仅行为表现下降，其内部的表示结构也发生了严重扰动，即使进行重训练也难以恢复原始状态。这才是真正的、结构性的抹除。理解这两种遗忘的本质差异，对于构建可靠的机器遗忘系统至关重要，尤其是在涉及隐私保护和安全性的应用中。</p>
<h3 id="揭示遗忘的深层机制表示空间诊断">揭示遗忘的深层机制：表示空间诊断</h3>
<p>为了系统性地探究大模型在遗忘过程中的内在变化，研究团队开发了一个统一的<strong>表示层分析工具箱</strong>，其核心组件包括：</p>
<ul>
<li><strong>PCA 相似度与偏移 (PCA Similarity &amp; Shift)</strong>：通过主成分分析（PCA），研究人员能够追踪模型表示空间的主要方向变化和数据分布中心的偏移程度。实验表明，对于可逆性遗忘，模型的表示空间在重学习后能够高度恢复到原始主方向，而不可逆性遗忘则导致表示方向的广泛漂移和数据分布的大尺度空间位移，难以还原。</li>
<li><strong>CKA (Centred Kernel Alignment)</strong>：这是一种衡量不同层之间表示空间结构相似性的工具。研究发现，在可逆性遗忘场景下，模型的层间结构几乎未受破坏，而不可逆性遗忘则导致CKA值迅速退化，显示出模型内部结构被严重破坏，层与层之间的信息关联性大幅降低。</li>
<li><strong>Fisher 信息矩阵 (FIM)</strong>：FIM从参数空间的角度提供了关于模型重要参数扰动程度的视角。通过分析特定层（如Layer 31）的Fisher分布，研究人员得以洞察模型遗忘过程中哪些关键参数发生了不可逆的变化，从而进一步验证了“结构性抹除”的理论。</li>
</ul>
<p>研究团队在多种遗忘方法（如GA、NPO、RLabel、GA+KL）、不同数据集（arXiv、GitHub、NuminaMath）和多种模型（Yi-6B、Qwen-2.5-7B）上进行了全面且深入的实证分析。实验结果清晰地表明，仅凭诸如成员推理攻击（MIA）成功率、遗忘样本准确率（F.Acc）和保留样本准确率（R.Acc）等表层指标，远不足以揭示模型遗忘的深层机制<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。在更复杂的推理任务（如MATH和GSM8K）上，即便任务复杂度提高，仍能观察到“受控重学习”在可逆场景下能带来准确率恢复，甚至有时超越初始性能，这提示了遗忘过程可能带来意想不到的“隐式增强效果”，例如作为一种对比式正则化或课程学习。</p>
<h3 id="伦理治理与ai的记忆未来">伦理、治理与AI的“记忆”未来</h3>
<p>这项研究的发现，对于AI伦理、数据隐私和模型治理具有深远影响。如果模型在表面上“遗忘”了数据，但其核心结构并未改变，那么这种“伪遗忘”将构成重大的隐私与安全隐患。用户或监管机构所要求的“被遗忘权”将难以真正落实，因为敏感信息可能随时通过重学习或其他方式被“召回”。</p>
<p>此项工作为我们提供了一套前所未有的工具，以诊断和理解大模型的遗忘行为。它不仅能够揭示模型是否真正“忘记”，甚至可以定位破坏发生的位置。这为未来设计**“可控、局部、不可逆”**的安全遗忘机制奠定了坚实基础。这意味着，研究人员和开发者将能够更精确地构建机器遗忘算法，确保数据从模型中真正被清除，而不是仅仅被抑制。</p>
<p>然而，挑战依然存在。如何将这些表示层诊断工具从研究实验室推广到实际的生产环境，如何在保证遗忘彻底性的同时，最大限度地减少对模型其他能力的损害，以及如何在海量复杂数据中实现高效的“局部遗忘”，都将是未来机器遗忘领域需要持续攻克的难题。这项研究无疑是朝着构建更负责任、更可信赖的人工智能迈出的重要一步，它迫使我们重新思考：机器的“记忆”究竟意味着什么，以及我们如何才能真正掌控它。</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>新智元（2025/6/16）。<a href="https://mp.weixin.qq.com/s/V2M5w0ImgIKT5kPmsLjz1Q">模型遗忘不代表记忆抹除，首次系统发现「可逆性遗忘」背后规律</a>。新智元。检索日期2025/6/16。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>36氪（2025/6/16）。<a href="https://www.36kr.com/p/3338556756486409">模型遗忘不代表记忆抹除，首次系统发现「可逆性遗忘」背后规律</a>。36氪。检索日期2025/6/16。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">大语言模型</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%9C%BA%E5%99%A8%E9%81%97%E5%BF%98/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">机器遗忘</a>
   </li>
  
   <li class="list di">
     <a href="/tags/ai%E4%BC%A6%E7%90%86/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">AI伦理</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9A%90%E7%A7%81/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">数据隐私</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">表示学习</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%89%E5%85%A8/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">人工智能安全</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%8F%AF%E9%80%86%E9%81%97%E5%BF%98/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">可逆遗忘</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E7%BB%93%E6%9E%84%E6%80%A7%E6%8A%B9%E9%99%A4/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">结构性抹除</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3"></p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/articles/google-notebooklmaiopenai-20250616123004/">Google NotebookLM：当AI成为你的专属知识策展人，连OpenAI也为之侧目</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/transformereso-lm65-20250616123004/">超越Transformer：混合扩散模型Eso-LM以65倍速重塑语言生成范式</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/agi-20250616083004/">超越参数堆叠：复旦邱锡鹏教授力推“情境智能”，探索通往AGI的下一幕</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/aidia-20250616003004/">AI原生浏览器Dia：是浏览器革新，还是通往“系统”的起点？</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/sam-altmanai/">揭秘“一茶匙水”的真相：Sam Altman的AI用水量宣言背后</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/aisiriwwdc-2025/">苹果在AI竞赛中艰难前行：Siri为何在WWDC 2025上“被缺席”？</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/2025-06-11-article-497/">十亿美元AI折戟儿童谜题：苹果研究揭示大型模型“思考幻象”背后的深层警示</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/2025-06-10-article-495/">AI“思考的幻觉”：当十亿美元模型被孩童谜题击败，我们该如何重新审视AI的承诺？</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/2025-06-10-article-489/">Meta的“超级智能”野望：AI重组与百亿级战略投资的深层剖析</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/" >
    &copy;  AI内参 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
