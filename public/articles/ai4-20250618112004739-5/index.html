<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>多模态AI的数学困境：从图像到形式化证明，准确率仅4%揭示深层推理鸿沟 | AI内参</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="香港科技大学团队发布的MATP-BENCH基准测试显示，当前多模态大模型（MLLMs）在理解图文结合的数学问题并将其形式化方面表现尚可（45%成功率），但在构建完整、可验证的形式化证明时，其成功率骤降至仅4%，暴露出模型在严谨逻辑推理和辅助线构造等深层能力上的显著不足，这指明了AI在迈向真正智能道路上的关键瓶颈。">
    <meta name="generator" content="Hugo 0.147.8">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    <meta name="author" content="温故智新AIGC实验室">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  
    <link rel="stylesheet" href="/css/custom.css">
  

  
    <link rel="stylesheet" href="/css/article-enhancements.css">
  


    


    
      

    

    

    
      <link rel="canonical" href="http://192.168.50.247:1313/articles/ai4-20250618112004739-5/">
    

    <meta property="og:url" content="http://192.168.50.247:1313/articles/ai4-20250618112004739-5/">
  <meta property="og:site_name" content="AI内参">
  <meta property="og:title" content="多模态AI的数学困境：从图像到形式化证明，准确率仅4%揭示深层推理鸿沟">
  <meta property="og:description" content="香港科技大学团队发布的MATP-BENCH基准测试显示，当前多模态大模型（MLLMs）在理解图文结合的数学问题并将其形式化方面表现尚可（45%成功率），但在构建完整、可验证的形式化证明时，其成功率骤降至仅4%，暴露出模型在严谨逻辑推理和辅助线构造等深层能力上的显著不足，这指明了AI在迈向真正智能道路上的关键瓶颈。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2025-06-18T11:20:04+08:00">
    <meta property="article:modified_time" content="2025-06-18T11:20:04+08:00">
    <meta property="article:tag" content="多模态大模型">
    <meta property="article:tag" content="形式化证明">
    <meta property="article:tag" content="人工智能">
    <meta property="article:tag" content="逻辑推理">
    <meta property="article:tag" content="MATP-BENCH">
    <meta property="article:tag" content="几何学">

  <meta itemprop="name" content="多模态AI的数学困境：从图像到形式化证明，准确率仅4%揭示深层推理鸿沟">
  <meta itemprop="description" content="香港科技大学团队发布的MATP-BENCH基准测试显示，当前多模态大模型（MLLMs）在理解图文结合的数学问题并将其形式化方面表现尚可（45%成功率），但在构建完整、可验证的形式化证明时，其成功率骤降至仅4%，暴露出模型在严谨逻辑推理和辅助线构造等深层能力上的显著不足，这指明了AI在迈向真正智能道路上的关键瓶颈。">
  <meta itemprop="datePublished" content="2025-06-18T11:20:04+08:00">
  <meta itemprop="dateModified" content="2025-06-18T11:20:04+08:00">
  <meta itemprop="wordCount" content="72">
  <meta itemprop="keywords" content="多模态大模型,形式化证明,人工智能,逻辑推理,MATP-BENCH,几何学,AI瓶颈,计算机科学">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="多模态AI的数学困境：从图像到形式化证明，准确率仅4%揭示深层推理鸿沟">
  <meta name="twitter:description" content="香港科技大学团队发布的MATP-BENCH基准测试显示，当前多模态大模型（MLLMs）在理解图文结合的数学问题并将其形式化方面表现尚可（45%成功率），但在构建完整、可验证的形式化证明时，其成功率骤降至仅4%，暴露出模型在严谨逻辑推理和辅助线构造等深层能力上的显著不足，这指明了AI在迈向真正智能道路上的关键瓶颈。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
  
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://192.168.50.247:1313/images/default%20%287%29.png');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        <img src="/logo/logo.png" class="w100 mw5-ns" alt="AI内参" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="">
              首页
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/articles/" title="">
              洞察
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/summary/" title="">
              综述
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/explore/" title="">
              主题探索
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about/" title="">
              关于
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref helvetica tracked">
        <span style="color: #999;">2025-06-18 11:20</span>
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">多模态AI的数学困境：从图像到形式化证明，准确率仅4%揭示深层推理鸿沟</h1>
      
      <p class="tracked"><strong>温故智新AIGC实验室</strong>
      </p>
      
      
    </header>
    <div class="article-content nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><blockquote>
<p>香港科技大学研究团队新推出的MATP-BENCH基准测试揭示，尽管多模态大模型（MLLMs）能较好地理解包含图像的数学问题，但在将这些理解转化为可验证的形式化证明时，其准确率骤降至仅4%，暴露了AI在严谨逻辑推理上的根本性瓶颈。</p></blockquote>
<p>在人工智能领域，大型语言模型（LLMs）的飞速发展已使其在处理文本信息方面取得了令人瞩目的成就。然而，人类智能的真正体现，往往超越了纯粹的语言范畴，融入了对视觉、听觉等多模态信息的综合感知与推理。特别是在数学和科学领域，图形、图表等视觉元素不仅是辅助理解的工具，更是构建严谨逻辑证明的直觉来源。那么，当下的多模态大模型（MLLMs）能否像人类数学家一样，从图文中汲取信息，并完成机器可严格验证的形式化证明（Formal Proof）呢？香港科技大学的研究团队与合作者推出的<strong>MATP-BENCH</strong>基准测试，给出了一个清醒的答案：我们距离实现这一目标，仍有漫长的路要走。</p>
<h3 id="挑战几何直觉matp-bench的诞生与设计理念">挑战几何直觉：MATP-BENCH的诞生与设计理念</h3>
<p>长期以来，自动定理证明（ATP）的研究主要聚焦于纯文本形式的定理，取得了显著进展。然而，现实世界的数学问题，尤其是几何学，其表达和理解离不开视觉元素。人类凭借对图形的直觉，能够迅速把握关键信息并指引证明过程。MATP-BENCH正是为了填补这一空白而生，它旨在系统性地评估MLLMs在<strong>多模态自动定理证明</strong>方面的真实能力 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>MATP-BENCH作为首个专为多模态定理证明设计的基准，具有多项核心特点：</p>
<ul>
<li><strong>多模态上下文：</strong> 每个问题都由一张图像和一个自然语言陈述构成，两者互为补充，共同提供完整的定理信息。这要求模型不仅能处理文本，更要从图像中提取未明确表达的几何关系（例如，通过图像判断线段平行或垂直，而不仅仅是文本描述）。</li>
<li><strong>多层次与多样性：</strong> 基准共包含1056个多模态定理，题目难度涵盖高中、大学和竞赛三个级别，内容上则覆盖了平面几何、3D几何、解析几何等多个领域。这种设计旨在全面评估模型在不同复杂程度和知识广度下的表现。</li>
<li><strong>多语言形式化：</strong> 所有定理都提供了在<strong>Lean 4、Coq 和 Isabelle</strong>这三种主流形式化证明辅助工具中的形式化版本。这些语言具有高度的逻辑严谨性，确保了证明的机器可验证性，同时也使得评估结果更具通用性。</li>
</ul>
<p>为了精准定位模型的瓶颈，MATP-BENCH设置了两个关联的核心任务：</p>
<ol>
<li><strong>多模态自动定理证明 (Multimodal Automated Theorem Proving)：</strong> 模拟人类专家从图文理解到端到端生成形式化定理及证明的全过程。</li>
<li><strong>多模态定理形式化 (Multimodal Theorem Formalization)：</strong> 仅评估模型将多模态信息理解并翻译为形式化定理陈述的能力，不要求生成完整证明。</li>
</ol>
<p>这项基准与现有纯文本或仅侧重“自动形式化”的基准（如miniF2F、ProofNet、LeanEuclid等）形成了鲜明对比，它聚焦于MLLM从零开始构建可验证证明的端到端能力 <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<h3 id="揭示瓶颈大模型在严谨推理面前的真实表现">揭示瓶颈：大模型在严谨推理面前的真实表现</h3>
<p>MATP-BENCH的实验结果清晰地描绘了当前MLLM在形式化定理证明上的能力边界和核心瓶颈。最令人警醒的发现是，模型的<strong>主要瓶颈并非“看懂题目”，而是后续“构建证明”的复杂逻辑推理过程</strong> <sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<ul>
<li><strong>理解与证明的巨大鸿沟：</strong> 以Lean 4语言为例，模型在任务二（仅形式化定理）上的平均pass@10成功率（即10次尝试内成功一次的概率）可达<strong>45.16%</strong>。这表明，MLLMs已具备相当不错的图文理解和形式化转译能力，能够将视觉和文本信息转化为形式化语言可理解的数学命题。然而，在需要完整证明的任务一上，这一数值骤降至仅<strong>4.26%</strong>。这一巨大落差直观地揭示了当前MLLMs在<strong>严谨逻辑推理和证明生成</strong>方面的深层缺陷。</li>
<li><strong>难度与准确率的负相关：</strong> 模型的性能随着题目难度的增加而显著下降。在Lean 4的任务一中，模型在高中、大学和竞赛级别问题上的平均成功率分别为6.39%、2.85%和1.29%，这表明复杂逻辑和抽象思维是模型的巨大挑战 <sup id="fnref3:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
<li><strong>形式化语言的差异与错误模式：</strong> 实验显示，模型在不同形式化语言上的性能存在差异。令人意外的是，在生成Coq语言上，某些模型的任务一成功率达到了12.15%，显著高于Lean 4和Isabelle。研究者推测，这可能与Coq更成熟的策略库、丰富的数学资源以及其命令式策略风格更适合大模型学习有关 <sup id="fnref4:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。
<ul>
<li><strong>不同模型的错误类型也揭示了其能力层次。</strong> 表现较好的闭源模型（如Claude-3.7和GPT-4.1）的错误主要集中在“无效或未完成的证明步骤”和“缺失前提/隐藏假设”，这意味着它们理解了基本概念，但无法完成复杂逻辑跳跃或发现隐含条件。而一些开源模型（如Qwen2.5-VL）则暴露出更多“基础性的生成错误”，例如“不正确或未声明的变量/定义”以及“缺失/错误的库导入”，表明它们甚至在掌握形式化语言的基本语法和规范上就已困难重重 <sup id="fnref5:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
</ul>
</li>
<li><strong>辅助线难题：</strong> 在人类几何解题中，添加辅助线是一种常见且强大的策略，它需要高度的直觉、创造性和前瞻性思维。MATP-BENCH的实验发现，随着题目难度的增加，需要辅助线的问题比例也显著上升。尽管模型能够尝试引入辅助线等构造性步骤，但它们几乎<strong>无法有效构造和利用辅助线来推进证明</strong>，导致包含辅助线构造的证明成功率极低 <sup id="fnref6:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。这凸显了当前AI在处理需要非显式信息或创造性构造才能解决的问题时的局限性。</li>
</ul>
<p>这些结果与更广泛的多模态大模型评估趋势相符。例如，小红书和上海交通大学发布的WorldSense基准也指出，现有MLLMs在音视频融合和理解方面表现不佳，甚至某些开源模型的准确率与随机猜测相当 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。知乎上关于2024年多模态大模型综述的文章也强调，MLLM的“推理”（Reasoning）能力是超越“理解”（Understanding）的更高层次挑战，它涉及执行复杂的跨模态推理 [^4, ^5]。</p>
<h3 id="超越表面理解ai迈向真正智能的深远路径">超越表面理解：AI迈向真正智能的深远路径</h3>
<p>MATP-BENCH的研究结果清晰地表明，尽管MLLMs在处理多模态信息方面取得了进步，但它们远未达到人类在严谨数学推理和证明构建上的能力。这种理解和推理之间的巨大鸿沟，对于AI的未来发展具有深远影响。</p>
<p>当前的MLLMs更像是一种高级的模式识别器和信息转换器，而非真正的逻辑推理者。它们能够识别图像中的几何实体，将其转化为符号表达，但在构建从前提推导结论的逻辑链条时，却步履维艰。这种局限性不仅仅是数学问题，它意味着在任何需要严谨、可验证逻辑推理的领域，如软件验证、医疗诊断、法律判决甚至科学发现，我们都不能盲目依赖当前的AI系统。一旦AI的输出涉及高风险决策，其背后的逻辑透明度和可信度就变得至关重要。</p>
<p>要让MLLM成为合格的多模态定理证明者，研究需要重点关注以下方向：</p>
<ul>
<li><strong>提升符号推理能力：</strong> 这要求开发新的模型架构或训练方法，专门增强模型在严谨的形式化逻辑系统中的推理和证明构建能力。这可能涉及到对经典符号AI方法和深度学习方法的更深层次融合，或者探索像链式思维（M-CoT）和上下文学习（M-ICL）这类旨在提升推理链条的方案 <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>。</li>
<li><strong>增强视觉-符号联合推理：</strong> 让模型不仅能“看见”图中的几何关系，更能将其无缝转化为可用于证明的形式化符号语言。这意味着需要更强大的跨模态对齐和融合机制，使得视觉特征能够直接、准确地驱动符号操作。</li>
<li><strong>探索交互式证明生成：</strong> 借鉴人类与证明辅助工具（Proof Assistants）交互的模式，允许模型利用外部工具进行辅助思考，或在证明过程中接受人类的指导和反馈。这种人机协作的模式，可能是一个充满希望的研究方向，尤其是在自动化程度尚未达到要求但又急需辅助的复杂场景中 <sup id="fnref7:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
</ul>
<p>MATP-BENCH的发布，不仅为多模态自动定理证明领域提供了一个重要的评估工具，更像是一面镜子，清晰地映照出当前大模型能力与人类智能之间的核心差距。它提醒我们，真正的通用人工智能，不仅仅是拥有庞大的参数和处理多模态数据的能力，更在于其是否能掌握逻辑的本质，进行严谨的、可验证的推理。这是AI研究的下一个前沿，也是决定AI能否真正赋能人类高阶认知任务的关键所在。</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>LRST（2025/6/18）。<a href="https://www.36kr.com/p/3341375396149762">形式化证明迈向多模态，MLLM正确率仅4%，港科大等推出全新基准</a>。36氪。检索日期2025/6/18。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref4:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref5:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref6:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref7:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>每时AI（2025/6/18）。<a href="https://mmssai.com/archives/18013">准确率最高只有48%？现有多模态大模型迎来大考!小红书&amp;上海交大发布WorldSense基准</a>。每时AI。检索日期2025/6/18。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>知乎（2025/6/18）。<a href="https://zhuanlan.zhihu.com/p/15363248761">2024多模态大模型综述</a>。知乎。检索日期2025/6/18。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">多模态大模型</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%BD%A2%E5%BC%8F%E5%8C%96%E8%AF%81%E6%98%8E/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">形式化证明</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">人工智能</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">逻辑推理</a>
   </li>
  
   <li class="list di">
     <a href="/tags/matp-bench/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">MATP-BENCH</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E5%87%A0%E4%BD%95%E5%AD%A6/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">几何学</a>
   </li>
  
   <li class="list di">
     <a href="/tags/ai%E7%93%B6%E9%A2%88/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">AI瓶颈</a>
   </li>
  
   <li class="list di">
     <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">计算机科学</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3" style="color: #666;">相关阅读</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/articles/article-20250617193006116-1/">游戏之智：小模型如何通过像素世界解锁通用推理能力</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/aitob-20250618112004732-4/">AI浪潮下的韧性与变革：为何ToB“老炮”仍能屹立不倒？</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/openai2ai-20250618112004726-3/">OpenAI与美国国防部的2亿美元合约：技术前沿、伦理边界与AI战场的未来</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/article-20250618102004312-0/">智能涌现：吴德周的AI眼镜突围战——驶向垂直场景的“佳明”之路</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/ipoai-20250618092004299-0/">从资本宠儿到IPO困兽：明略科技在AI浪潮中的突围困境</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/article-20250618092004306-1/">超越画笔：人工智能生成内容的著作权迷宫与未来之路</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/minimax-m1ai-20250618082004316-0/">MiniMax M1的非共识之路：中国大模型公司如何重塑AI推理的边界</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/openai65ioai-20250618082004323-1/">OpenAI豪掷65亿美元收购io：AI硬件的黎明，抑或又一个起点？</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/article-20250618082004330-2/">全球AI人才流动的悖论：中国人才基石的挑战与机遇</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/article-20250618082004337-3/">面部识别技术：急速扩张下的监管真空与伦理困境</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/gemini-25aiopenai-20250618062004410-0/">谷歌Gemini 2.5：以“思考”模型重塑企业AI赛道，剑指OpenAI主导地位</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/metaai-20250618042004400-0/">Meta重金押注AI：一场关于数据、权力与“超级智能”的豪赌</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/metaai-20250618012004426-1/">Meta巨资押注AI：一场豪赌，还是市场重塑的序曲？</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/article-20250618002005001-0/">任正非的“无心之言”：华为长局中的中国AI芯未来</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/articles/article-20250617232005871-4/">情报领域的AI新前沿：中国情报机构重塑谍报战格局</a>
        </li>
	    
    </ul>
</div>



<div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links mt3">
  <p class="f5 b mb3" style="color: #666;">AI写作声明</p>
  <div class="f6 lh-copy" style="color: #777;">
    <p>本文由AI系统全流程生成，涵盖选题、资料整合与撰写。文章发布后由人工编辑进行核查。文末附有详细的[引用信息索引]，供您查证溯源。</p>
  </div>
</div></aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://192.168.50.247:1313/" >
    &copy;  AI内参 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

    <script src="/js/relative-time.js"></script>
  </body>
</html>
