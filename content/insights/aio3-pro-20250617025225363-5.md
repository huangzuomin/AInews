---
title: "当“推箱子”邂逅AI：o3-pro在经典游戏基准测试中突破上限"
date: 2025-06-17T02:52:25+08:00
draft: false
summary: "o3-pro大模型在名为Lmgame的新基准测试中，成功通关经典游戏“推箱子”并无限畅玩“俄罗斯方块”，突破了现有AI在该领域的能力上限。这一突破揭示了大模型在复杂规划、长期推理和环境交互方面的显著进步，同时其操作耗时也凸显了当前AI效率的瓶颈，为通用人工智能的发展提供了新的评估维度和研究方向。"
tags: 
  - 人工智能
  - 大模型
  - 基准测试
  - 推箱子
  - 俄罗斯方块
  - Lmgame
  - "o3-pro"
  - 通用人工智能
main_topics: 
  - AI能力评估
  - 大模型应用拓展
  - 智能体机制
---

> o3-pro大模型在名为Lmgame的新基准测试中，成功通关经典游戏“推箱子”并无限畅玩“俄罗斯方块”，突破了现有AI在该领域的能力上限，这标志着大模型在复杂规划、长期推理及环境交互方面取得了显著进展，尽管其操作耗时揭示了效率瓶颈。

在人工智能领域，衡量模型的“智能”水平始终是一项复杂的挑战。传统上，我们依赖于语言理解、逻辑推理等任务来评估大模型（LLMs）的能力。然而，一系列人类熟知的怀旧小游戏，如“推箱子”和“俄罗斯方块”，如今正成为大模型能力的新兴战场。近期，一个名为Lmgame的开放式基准测试平台崭露头角，而o3-pro模型在此次竞赛中表现出惊人的实力，不仅在“推箱子”游戏中突破了现有测试关卡上限，更在“俄罗斯方块”中展现出近乎无限的持续游戏能力，为我们理解大模型的决策与规划能力提供了新的视角。

### **从怀旧游戏到大模型新测度：Lmgame的崛起**

Lmgame基准测试的设计理念是将大型语言模型置于动态、需要持续交互的游戏环境中，以评估它们在多步规划、状态感知和适应性行为方面的表现。这与传统的静态问答或文本生成任务截然不同，它更接近于我们对“通用智能”的想象——在复杂环境中理解、决策并执行的能力。

此次o3-pro挑战的“推箱子”游戏，基于1989年的经典版本进行修改，要求玩家在迷宫中推动箱子到达指定位置。在此之前，这项测试的评估指标是游戏结束前推动到目标位置的箱子总数。但o3-pro的表现令人瞩目，它不仅完成了所有当前设定的关卡，甚至给人一种“卷面只有一百分，是因为总分只有一百分”的感觉 [^1]。这促使Lmgame团队表示，将动态更新游戏地图，引入更多关卡以适应模型的进步。在“推箱子”项目中，o3-pro的成绩是前SOTA（State-of-the-Art）模型o3的两倍。

另一款挑战项目“俄罗斯方块”，其得分计算方式是将放置的方块数量与清除行数的10倍相加，直到游戏结束。o3-pro在这一挑战中展现出“根本停不下来”的连续游戏能力 [^1]，这无疑证明了其在实时决策和适应性策略上的强大。值得注意的是，这些经典游戏对人类而言，除了规则理解，更需要直觉、空间推理和快速反应，而大模型通过其复杂的内部机制来模拟并超越这些能力，无疑是AI发展的重要里程碑。

Lmgame基准测试不仅限于这两款游戏。它还包含了其他四款多样化的游戏，包括策略性强的“2048”、消除类游戏“糖果传奇”、平台跳跃游戏“超级马里奥兄弟”以及逻辑推理和证据分析的“逆转裁判”。每款游戏都设有独特的评估标准：

*   **超级马里奥兄弟：** 衡量马里奥在所有关卡中累积的水平移动距离，直到失去所有生命或完成最终关卡。
*   **2048：** 评估所有合并方块值的总和，以2为底的对数乘10为最终分数。
*   **糖果传奇：** 在固定50回合内消除的糖果总数。
*   **逆转裁判：** 通过所有案件关卡中正确动作（例如提交证据、选择对话）的总计数来衡量。

这些多样化的游戏类型旨在全面评估大模型在不同认知维度上的能力，从快速反应到长期规划，从抽象推理到信息整合。

### **探究幕后机制：AI的“思考”与“行动”**

o3-pro在Lmgame中的成功，并非简单的“巧合”或“蛮力”，而是其底层架构和交互机制优化的结果。Lmgame的测试过程采用了一种**迭代交互循环模式**。在这种模式下，游戏环境会持续向大模型提供最新的游戏状态，模型则根据这些状态生成动作，这些动作随即在游戏环境中被执行，并根据执行结果计算奖励。随后，游戏状态会更新，为模型的下一轮决策提供依据。这种循环机制，与强化学习（Reinforcement Learning）中的智能体与环境交互模式高度相似，使大模型能够在一个动态系统中进行探索和学习。

为了辅助大模型进行更有效的决策，Lmgame还引入了**智能体框架**，其中包含了**感知、记忆和推理**等模块 [^1]。这意味着大模型不仅仅是简单地接收输入并输出动作，它能够：
*   **感知**：解析游戏画面和状态信息。
*   **记忆**：存储过去的行动序列和状态信息，帮助理解游戏历史和连贯性。
*   **推理**：基于当前感知和过往记忆，进行逻辑分析和多步规划，以生成最优的下一步动作。

此外，为了确保评估结果的稳定性和可比性，该模式还实施了**提示标准化**，以减少提示词（prompts）带来的性能波动。这对于量化评估而言至关重要，它确保了模型的表现是其内在能力的体现，而非对特定提示词的敏感性。

然而，o3-pro的成功并非没有代价。尽管其在完成任务上表现出色，但其操作相当耗时，每走一步可能都需要数分钟 [^1]。这一点暴露了当前大模型在游戏场景中的一个核心挑战：**效率**。尽管它们可能具备强大的规划和推理能力，但推理速度仍远不及人类的直觉和瞬时反应。这一瓶颈在诸如“俄罗斯方块”这类需要高速决策的游戏中尤为明显，尽管o3-pro能“无限玩下去”，但其每一步的缓慢执行使其无法真正模拟人类的流畅游戏体验。这促使一些网友提出，如果让大模型编写程序来玩这些游戏，而非直接挑战，或许能达到更好的效果 [^1]，这暗示了AI在不同智能形式（如编程逻辑与实时决策）间的潜在能力差异。

### **超越游戏：这项突破对通用AI意味着什么？**

Lmgame基准测试项目由UCSD的Hao AI Lab出品，该实验室隶属于USCD的机器学习系统实验室和NLP实验室，负责人是助理教授张昊。张昊教授背景深厚，本硕博分别就读于华南理工、上海交大和卡内基梅隆大学，并在UC伯克利从事博士后研究，他还参与创立了LMSYS，并担任大模型竞技场（LLM Arena）顾问 [^1]。LMSYS是一个非营利组织，其研发的大模型竞技场以及知名的模型框架SGLang、vLLM都在业界享有盛誉。Hao AI Lab目前也获得了谷歌和英伟达等巨头的资助，这进一步证明了其研究方向和成果的重要性。Lmgame的开源性质也意味着全球的研究者都可以下载并测试自己的模型，加速了该领域的研究进展。

o3-pro在经典游戏中的突破，不仅仅是“玩得好”那么简单，它对通用人工智能（AGI）的探索具有深远意义。传统上，AI在特定领域（如AlphaGo在围棋）表现卓越，但在通用性和泛化能力上仍有不足。像“推箱子”这样的游戏，需要智能体理解空间关系、进行多步前瞻性规划以及应对不可预见的障碍，这些能力对于构建能够适应真实世界复杂任务的AI至关重要。o3-pro的成功表明，当前的大模型正在从单纯的文本生成器向更具通用问题解决能力的智能体迈进。

它也为评估AI系统提供了一个新的维度。传统的基准测试往往侧重于静态知识和语言理解，而游戏环境则能更全面地反映AI的动态决策、适应性和长期规划能力。当然，如前所述，效率问题仍是阻碍大模型在实时复杂场景中广泛应用的关键。解决这一问题，可能需要架构上的创新，使其能够在保持高推理质量的同时，显著提升决策速度。

展望未来，Lmgame团队表示，在网友的呼声下，他们将很快安排大模型挑战更复杂、更开放世界的游戏，例如“宝可梦”系列。值得一提的是，谷歌的Gemini模型已在今年5月初成功直播通关了“宝可梦·蓝”[^1][^2]，这预示着AI在游戏领域的探索将不断深入，从经典的2D解谜到开放世界的复杂策略，AI的智能边界正在被持续拓宽。每一次这样的突破，都不仅仅是游戏的胜利，更是我们理解和构建更智能机器的关键一步，也提醒我们思考，当AI在虚拟世界中展现出越来越强大的能力时，这些能力将如何映射到我们的现实世界，并带来何种深远影响。

## References
[^1]: 克雷西 (2025/6/16)。[o3-pro通关“推箱子”，人类怀旧小游戏成了大模型新Benchmark](https://www.36kr.com/p/3338821408911880)。36氪。检索日期2025/6/16。
[^2]: o3-pro通关“推箱子”，人类怀旧小游戏成了大模型新Benchmark (2025/6/16)。[o3-pro通关“推箱子”，人类怀旧小游戏成了大模型新Benchmark](https://finance.sina.com.cn/tech/csj/2025-06-16/doc-infafyah3201582.shtml?froms=ggmp)。新浪财经。检索日期2025/6/16。
