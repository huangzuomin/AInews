---
title: "AI“思考的幻觉”：当十亿美元模型被孩童谜题击败，我们该如何重新审视AI的承诺？"
date: 2025-06-10T16:40:06.439Z
draft: false
tags: ["人工智能", "大型语言模型", "AI推理", "Gary", "Marcus", "技术局限性", "AI伦理", "模式识别"]
categories: ["AI News"]
main_topics: ["AI推理能力的局限性", "AI领域过度宣传的风险", "AI未来发展方向"]
author: "AI News Assistant"
summary: "苹果公司近期研究揭示，大型语言模型在复杂推理任务上表现出明显局限，甚至在面对孩童都能解决的谜题时会“崩溃”，引发了对AI过度宣传的重新思考。文章深入探讨了当前AI在模式识别与真正推理之间的鸿沟，并分析了这种“思考的幻觉”可能带来的社会、伦理和经济风险，强调AI发展需从追求表面智能转向提升核心的可靠推理能力。"
cover:
  image: "https://i.guim.co.uk/img/media/27031e7bac66edeea7685425d9ec92234b680853/0_29_2000_1600/master/2000.jpg?width=465&dpr=1&s=none&crop=none"
  alt: "AI“思考的幻觉”：当十亿美元模型被孩童谜题击败，我们该如何重新审视AI的承诺？"
source_url: ""
word_count: 1711
reading_time: 9
ai_score: 0
sync_time: "2025-06-10T17:38:54.933Z"
---

## 📝 Summary

苹果公司近期研究揭示，大型语言模型在复杂推理任务上表现出明显局限，甚至在面对孩童都能解决的谜题时会“崩溃”，引发了对AI过度宣传的重新思考。文章深入探讨了当前AI在模式识别与真正推理之间的鸿沟，并分析了这种“思考的幻觉”可能带来的社会、伦理和经济风险，强调AI发展需从追求表面智能转向提升核心的可靠推理能力。

> 近期苹果公司发布的研究论文揭示，备受推崇的大型语言模型（LLMs）在处理复杂推理任务时表现出显著局限性，即便面对孩童都能解决的简单谜题也可能“崩溃”。这一发现挑战了当前AI领域的过度宣传，强调了AI在模式识别与真正推理能力之间的鸿沟，促使行业重新审视AI的实际能力及其未来发展方向。

### “思考的幻觉”：AI推理能力的真实边界

长期以来，人工智能领域弥漫着一种乐观情绪，认为大型语言模型（LLMs）正迅速逼近甚至超越人类的认知能力。然而，最近由苹果公司发布的一项研究，如同一股清流，正冲击着这一主流叙事，揭示了当前AI能力中一个关键的**“思考的幻觉”**[^1]。该研究指出，尽管ChatGPT、Claude和Deepseek等领先模型看起来“聪明”，但在面对需要真正推理和多步逻辑的复杂任务时，它们往往“崩溃”[^1]。

这一发现并非孤立，而是得到了像纽约大学荣休教授、资深AI评论员加里·马库斯（Gary Marcus）等长期持批判态度的专家的印证。风险投资家乔什·沃尔夫（Josh Wolfe）甚至创造了一个新词——“GaryMarcus’d”，来形容“批判性地揭露或揭穿人工智能被过度炒作的能力……通过强调其在推理、理解或通用智能方面的局限性”的行为[^1]。这凸显了业界对当前AI能力重新评估的紧迫性。

问题的核心在于，这些“数十亿美元”的AI系统，尽管在海量数据上进行了训练，并被**明确设计用于推理任务**，却在某种程度上陷入了**模式识别**的困境。它们擅长于捕捉数据中的统计关联和表面模式，这使得它们在生成看似流畅、逻辑一致的文本方面表现出色。然而，当任务超越了其训练数据中的已知模式，要求模型进行抽象、归纳或演绎，特别是面对需要多步逻辑链条和新颖情境的挑战时，它们的表现便捉襟见肘[^3]。

正如心理学今日（Psychology Today）的分析所指，在低复杂度的任务中，简单的LLMs可能表现良好；在中等复杂度下，推理模型则会闪光。但一旦认知负荷增加，需要抽象或多步逻辑时，AI的“大脑”似乎就出现了问题[^3]。这表明，当前的大型语言模型及其变体（如大型推理模型LRMs），尽管在形式上能够处理推理任务，但其底层机制仍与人类的**鲁棒性推理**存在本质差异。

### 重新审视AI的承诺与风险

苹果公司的研究并非是为了贬低AI的潜力，而是为了促使我们以更清醒的视角来看待AI的实际能力。当前，AI技术正以前所未有的速度渗透到社会各个角落，从医疗诊断到金融分析，从教育辅助到创意内容生成。如果这些被寄予厚望的AI系统，在关键的推理环节存在根本性缺陷，那么其带来的潜在风险将是巨大的。

首先，这种**“思考的幻觉”**可能导致人们对其能力的**过度信任**。在关键决策场景中，如果AI基于不充分或错误的推理得出结论，可能引发严重后果，例如错误的医疗建议、有偏见的法律判断，甚至引发社会信任危机。其次，对于投入了巨额资金和资源的AI公司而言，对模型能力的高估可能导致投资方向的偏差，将重心放在规模和参数增长上，而非真正提升模型的**通用智能和鲁棒性**。

从更广泛的社会和伦理角度来看，如果AI无法可靠地进行推理，那么关于其在伦理决策、自主武器系统、甚至司法审判中扮演角色的讨论就显得尤为迫切和复杂。我们必须深刻反思，在追求AI“智能”表象的同时，是否忽略了其核心的**可解释性、可靠性和可控性**。

加里·马库斯等人的批判声音，以及苹果公司的研究，提供了一个重要的警示：AI的进步并非简单地通过增加模型规模和数据量就能无限实现。未来的AI发展，需要将重心从单纯的模式匹配转向构建具备真正**因果理解、常识推理和学习适应新颖情境能力**的系统。这意味着可能需要融合符号AI、认知科学等不同范式的研究成果，探索更深层次的架构和学习机制。

此次研究，无疑是AI发展历程中的一个重要转折点。它促使我们从盲目乐观中抽身，重新审视AI的本质、局限性及其对社会可能造成的深远影响。只有正视这些挑战，AI才能真正从“思考的幻觉”走向真正、可靠的智能，从而为人类社会带来更可持续的福祉。

## 🏷️ Tags

#人工智能 #大型语言模型 #AI推理 #Gary #Marcus #技术局限性 #AI伦理 #模式识别

---

*📰 本文由 AI News Assistant 自动生成于 2025/6/11 00:40:06 (UTC+8)*
