---
title: 华为突破AI基础设施瓶颈：CloudMatrix384如何重塑超大规模计算范式
date: 2025-06-19T08:20:04+08:00
draft: false
featured_image: images/default (7).png
summary: "华为最新发布的CloudMatrix384 AI超级节点，在DeepSeek-R1大语言模型评估中展现出超越英伟达H800 GPU的计算效率。这一突破性成果得益于CloudMatrix384创新的统一总线（UB）网络架构和昇腾910C NPU的协同作用，为构建高性能、可扩展的AI原生数据中心树立了新标杆，并预示着全球AI算力格局的潜在转变。"
tags: 
  - 华为
  - AI算力
  - 昇腾910C
  - CloudMatrix384
  - 大语言模型
  - 英伟达
  - AI基础设施
  - 并行计算
main_topics: 
  - AI基础设施
  - 芯片技术
  - 算力竞争
---

> 华为最新发布的CloudMatrix384 AI超级节点及其昇腾910C NPU在运行DeepSeek-R1大语言模型时，展示出超越英伟达H800 GPU的计算效率，其核心在于创新的统一总线（UB）网络架构，为构建下一代AI原生数据中心提供了高性能、可扩展的解决方案，并预示着AI算力竞争格局的新变化。

在人工智能浪潮席卷全球的当下，算力已成为国家乃至企业竞争力的核心。长期以来，英伟达凭借其CUDA生态系统和领先的GPU技术，几乎垄断了高端AI芯片市场。然而，这一格局正被一股来自东方的力量——华为——悄然挑战。最近，华为联合硅基流动发布的一篇论文揭示了其AI超级节点CloudMatrix384在实际大语言模型（LLM）推理任务中，**效率超越英伟达H800 GPU的惊人表现**，这不仅是对其技术实力的强有力证明，也为全球AI基础设施的发展指明了新的方向 [^1]。

### 华为的“互联即力量”策略：CloudMatrix384的架构创新

当前，大型AI模型，特别是混合专家（MoE）模型，对计算、内存和通信提出了前所未有的要求。传统的AI数据中心架构，往往受限于孤立的计算单元、有限的内存带宽以及高昂的芯片间通信开销。华为的CloudMatrix架构，正是为解决这些深层痛点而生。

CloudMatrix的核心理念在于**“互联即力量”**，它彻底重新构想了AI数据中心基础设施。其愿景是拆除传统的分层设计，实现CPU、NPU、内存、网络接口卡（NIC）等所有异构系统组件的完全点对点分解和池化，并通过一个统一的、超高性能网络互联 [^2]。

这种架构的首次生产级落地便是CloudMatrix384超级节点。它集成了惊人的**384颗昇腾910C NPU和192个鲲鹏CPU**。最引人注目的技术亮点是其采用了**超高带宽、低延迟的统一总线（UB）网络**。与传统通过CPU中介或RDMA网络进行通信不同，UB网络实现了所有NPU和CPU之间的直接、高性能通信，从而将节点间通信性能提升至接近节点内水平。

具体来看，CloudMatrix384的UB网络设计为无阻塞网络，在L2交换层没有带宽超额订阅。每个昇腾910C NPU提供高达392GB/s的单向UB带宽，而每个鲲鹏CPU插槽提供约160GB/s的单向UB带宽。这种极致的互联能力，对于通信密集型任务至关重要，例如大规模MoE模型的专家并行（Expert Parallelism）和分布式键值（KV）缓存访问。

昇腾910C NPU本身也采用了先进的双die封装技术，每个封装集成了两个计算die，共享128GB封装内存，并通过高带宽交叉die结构连接。其单颗芯片可提供约376 TFLOPS的BF16/FP16密集吞吐量，这为高效率计算奠定了硬件基础。

软件层面，华为构建了名为**神经网络计算架构（CANN）**的全面软件生态系统，类似于英伟达的CUDA。CANN作为中间件，将高级AI框架（如PyTorch）与昇腾NPU底层硬件高效集成，通过将计算图转换为优化的硬件可执行指令，简化了开发并最大化了性能。配合华为云的MatrixResource、MatrixLink等基础设施软件，CloudMatrix384的部署和资源编排得以实现无缝衔接 [^1]。

### 性能突破与市场格局：超越英伟达的背后

华为的CloudMatrix-Infer服务解决方案，旨在充分利用CloudMatrix384的强大能力，特别是针对DeepSeek-R1这样的大规模MoE模型。通过一系列核心创新，CloudMatrix-Infer取得了显著的性能提升。

其中最关键的创新包括：
*   **点对点服务架构**：将预填充、解码和缓存分解到独立可扩展的资源池中，并通过UB网络实现对缓存数据的高带宽、统一访问，从而减少数据局部性限制并提高缓存效率。
*   **大规模专家并行（LEP）策略**：利用UB网络实现高效的token调度和专家输出组合，支持极高的EP度数（如EP320），使每个NPU芯片仅托管一名专家，显著降低了解码延迟。
*   **硬件感知优化**：包括高度优化的算子、基于微批处理的流水线和INT8量化，以提高执行效率和资源利用率。

在对DeepSeek-R1模型的广泛评估中，CloudMatrix-Infer的计算效率确实令人瞩目。它在预填充阶段为每颗NPU提供了**6688 tokens/s的吞吐量**，在解码期间为每颗NPU提供了**1943 tokens/s的吞吐量**，同时保持每个输出token低于50ms的低延迟 [^1]。换算成计算效率，预填充阶段达到**4.45 tokens/s/TFLOPS**，解码阶段达到**1.29 tokens/s/TFLOPS**。这些数据不仅验证了华为的架构设计，更重要的是，**它超越了NVIDIA H100上的SGLang和H800上的DeepSeek等领先框架的公布效率** [^1]。

这一成就也印证了英伟达CEO黄仁勋早前的判断：尽管美国芯片技术可能领先华为一代，但人工智能是一个并行问题。如果单台计算机的性能不足够强，那就用更多的计算机。黄仁勋的言论间接肯定了华为通过大规模并行和高效互联来弥补个体芯片差距的策略。华为的这一进展，无疑增强了其在中国乃至更广阔市场满足大模型需求的能力，对英伟达在AI算力市场的长期主导地位构成了实质性挑战。

### AI基础设施的未来：技术融合与产业影响

CloudMatrix384的成功不仅仅是单一产品性能的提升，它更代表着AI基础设施设计理念的一次范式转变。随着模型规模的爆炸式增长，尤其是MoE架构的普及和上下文长度的不断扩展，传统的数据中心架构瓶士愈发明显。华为通过其点对点、完全互联、超高带宽的UB网络，为未来的AI数据中心基础设施树立了新标杆。

从更广阔的视角来看，这种“互联先行”的架构策略，预示着AI计算将越来越依赖于**分布式系统的整体优化，而非仅仅是单点芯片性能的提升**。这意味着未来的竞争将不仅仅是芯片硬件的算力比拼，更是系统级设计、网络拓扑、软件栈协同优化能力的全面较量。

CloudMatrix384的未来增强方向也描绘了一幅令人兴奋的蓝图：整合和统一VPC和RDMA网络平面以实现更简化的互连，扩展到更大的超级节点配置，以及追求更深入的CPU资源分解和池化。这些方向都指向了AI数据中心基础设施的更高灵活性、效率和可扩展性。

华为在AI算力领域的这一突破，不仅提升了其自身在全球科技舞台上的竞争力，也为其他国家和地区发展自主可控的AI基础设施提供了宝贵的经验。在当前地缘政治背景下，这不仅是技术上的胜利，更是战略上的主动出击，可能重塑全球AI生态系统的力量平衡。AI技术的发展，从来不只是纯粹的技术演进，它总是与经济、社会乃至国际关系紧密交织。华为CloudMatrix384的案例，正是这一复杂现实的最新注脚。

## References
[^1]: 智东西（2025/6/18）。[黄仁勋夸爆的华为AI超节点，技术秘籍披露，昇腾910C跑DeepSeek，效率超英伟达](https://m.36kr.com/p/3341960690612743)。36氪。检索日期2025/6/19。
[^2]: 李水青（2025/6/18）。[黄仁勋夸爆的华为AI超节点，技术秘籍披露！昇腾910C跑DeepSeek](https://news.qq.com/rain/a/20250618A08SEA00)。腾讯新闻。检索日期2025/6/19。
