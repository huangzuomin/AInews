---
title: 大型语言模型的幻象：苹果争议揭示通用智能之路的挑战
date: 2025-06-21T18:10:04+08:00
draft: false
featured_image: "https://oss.zhidx.com/8bf8bc966f150427a10a6817f78bcd35/6855f600/uploads/2025/06/68512c3263371_68512c325f12f_68512c325f108_VLM.png"
summary: 苹果公司一篇质疑大型语言模型（LLM）推理能力和存在“准确率崩溃”的论文，在AI社区引发了激烈辩论，挑战了“规模化即一切”的行业信念。尽管面临来自AI专家和AI模型Claude本身的驳斥，但纽约大学教授加里·马库斯反驳了这些质疑，并获得了Salesforce和UC伯克利研究的间接支持，这些研究揭示了LLM在多轮推理和视觉理解上的脆弱性与隐私问题，促使业界重新思考AI的评估范式和神经符号结合等未来架构方向。
tags: 
  - 大型语言模型
  - AI推理
  - 苹果AI
  - Gary Marcus
  - Claude
  - Salesforce AI
  - 视觉语言模型
  - AI伦理
  - AI评估
  - 神经符号AI
main_topics: 
  - 前沿模型与算法
  - AI伦理与治理
  - 产业生态与商业版图
---

> 苹果公司最近的一篇论文，对大型语言模型（LLM）在复杂推理任务上的“准确率崩溃”提出质疑，引发了硅谷AI圈的广泛争论。这场争论不仅暴露了当前模型能力的深层局限性，也呼唤着对人工智能评估范式和未来架构的重新思考。

在AI领域狂飙突进的时代，关于通用人工智能（AGI）何时到来以及如何实现的讨论从未停歇。然而，最近来自科技巨头苹果公司的一篇论文，犹如一道冷峻的审视目光，直指当前大型语言模型（LLM）的“思考”能力，质疑其在处理复杂难题时并非真正的推理，而是“死记的模式机器”在特定阈值后出现“准确率崩溃”。这一观点如同投入平静湖面的石子，迅速激起了业界轩然大波，引来多方“围攻”，却也意外地获得了来自Salesforce和加州大学伯克利分校等机构的间接支持，将一场技术论辩推向了对AI本质和评估范式的深刻反思。

### 苹果的“暴论”与即时反弹

苹果公司上周发表的论文，核心论点在于大型推理模型在超出特定复杂度（例如，河内塔游戏圆盘数量增多）时，其准确率会急剧下降，仿佛“思考能力”瞬间瓦解。这无疑挑战了当前业界普遍奉行的“规模化即一切”的信仰。

反驳的声音接踵而至，其中不乏直接而尖锐的批评。Anthropic旗下的大型语言模型Claude甚至被其人类合作者Lawsen作为第一作者，在arXiv上“发表”了一篇名为《思维的幻觉的幻觉》的论文，直接驳斥苹果的实验设计。Claude团队通过对苹果论文中的河内塔实验进行分析，指出模型所谓的“推理崩溃”主要源于实验设计的局限性，而非根本性的推理失败。他们提出三个关键问题：首先，实验在报告失败点系统性地超出了模型的输出Token限制，而模型本身已在输出中承认了这些限制；其次，作者的自动评估框架未能区分推理失败与实际约束；最后，某些基准测试包含了数学上不可能解决的问题，却仍将模型未能解决这些问题判定为失败。当这些实验缺陷被修正后，初步实验表明，此前被报告为完全失败的河内塔实例仍能保持高准确率。这一“AI参与学术讨论”的现象本身，就引发了科技圈的广泛关注和热议，有人赞叹AI已成为学术界的重要参与者，也有人质疑AI的实际贡献。

### 深入剖析：实验设计与模型本质之争

面对潮水般的反驳，纽约大学名誉教授、《代数思维》和《深度学习正在遭遇瓶颈》的作者**加里·马库斯（Gary Marcus）**，这位长期以来对深度学习局限性持批判态度的学者，扮演了重要的辩论角色。他总结了针对苹果论文的七大反驳观点，并逐一进行了驳斥，坚定地站在了苹果的立场。

马库斯认为，许多反驳看似有理，实则缺乏说服力。例如，关于“Token限制导致结果失真”的观点，他承认部分属实，但强调这本身就是LLM的一个**“Bug”而非“特性”**。他指出，许多LLM在8个圆盘的河内塔问题上失败，而其最优解（255步）完全在大多数模型的Token限制之内，这表明问题并非完全出在输出长度上。此外，他尖锐地指出，如果LLM连河内塔这样“基本”的计算都无法可靠完成，又如何能被寄予厚望去处理军事战略或分子生物学等更复杂、更不可预测的问题？这无疑重新聚焦了对模型_可靠性_和_泛化能力_的核心关切。

对于“更大的模型可能会做得更好”这一论调，马库斯驳斥道，虽然部分大型模型可能在特定问题上有所改进，但这并不能解决根本问题。他强调，苹果的研究揭示的是一种**“假象”**：模型可能在较小规模问题上表现出色，给人以“精通”的错觉，但在问题复杂度稍有增加时便会**“崩溃”**。他形象地将此比喻为“掷骰子游戏”，暗示了当前“规模至上”策略的不确定性和不可靠性。

马库斯还特别提及了“系统可以用代码解决难题”这一反驳。他认为这恰恰是对**神经符号人工智能（Neuro-Symbolic AI）**的巨大支持——即需要整合神经网络和符号算法及表示（如逻辑、代码、知识图谱）的方法。但他同时指出，苹果论文的目的是探究LLM在**无人协助下**如何通过推理和回溯探索解决方案，而非其利用现有代码的能力。这就像学生在数学考试中需要手算来证明对概念的理解，而不是简单地利用计算器给出答案。

> “苹果论文的真正要点是，随着算法复杂度和与训练分布的距离不断增加，大语言模型不再适合用来运行算法，就像人类不应该充当计算器一样。如果我们想要实现AGI，就必须做得更好。”马库斯如是说。

### 旁证与更深层次的担忧

令人玩味的是，在苹果论文引发争议的同时，其他独立研究也从不同角度印证了其核心担忧，为这场论战增添了更多佐证。

**Salesforce**于5月24日发布的一篇题为《CRMArena-Pro：对不同业务场景和互动中的大语言模型智能体进行全面评估》的论文[^1]，便与苹果的观点不谋而合。该研究提出了一个全新的基准测试CRMArena-Pro，用于评估LLM智能体在真实商业场景中的表现，涵盖销售、服务等多种流程。结果令人沮丧：领先的LLM在单轮任务中的成功率仅为58%左右，而在需要复杂推理和算法精度的**“多轮”交互**下，其性能显著下降至**约35%**。更值得警惕的是，该论文指出，这些智能体的**固有保密意识几乎为零**。这不仅揭示了当前LLM在处理复杂业务逻辑和长期规划方面的能力不足，更对其在企业级应用中的_信息安全和隐私保护_提出了严峻挑战。

无独有偶，加州大学伯克利分校于6月9日发表的论文《隐藏在显而易见的地方：视觉语言模型忽略了它们的视觉表现》[^2]，则将质疑的矛头指向了**视觉语言模型（VLM）**。研究发现，VLM的性能明显低于其视觉编码器，在以视觉为中心的基准测试中，其表现甚至下降到接近偶然水平。这意味着VLM未能有效整合视觉和语言信息，它们往往倾向于_忽略来自视觉编码器的丰富信息_，而更多地依赖其大语言模型部分的语言先验，从而学会“捷径”而非真正的抽象推理。这揭示了多模态AI在跨模态信息融合上的深层脆弱性，进一步佐证了模型在复杂场景下并非真正“理解”的观点。

### 争议背后的启示：评估范式与未来方向

这场围绕苹果论文引发的学术论战，其影响已远远超越了单纯的技术细节争论，触及了整个大模型发展前景的**信仰之争**。一方面，苹果这种“反共识”的观点受到了来自多方的围攻，这反映了当前AI领域对“规模化”路径的路径依赖和乐观预期；另一方面，Salesforce和UC伯克利的研究则从多轮复杂推理任务的显著低成功率，以及视觉语言模型对视觉信息利用的脆弱性等不同角度，提供了有力的佐证，使得这场争论变得更具说服力。

这场争论不仅指出了“规模化”路径的潜在局限，更深刻地**呼唤评估范式的革新与底层架构的突破**。当前的许多AI基准测试可能过于简化，未能真实反映模型在复杂、开放世界中的表现。我们需要设计更能真实反映智能本质的测试基准，尤其是在涉及多步推理、长期记忆、以及跨模态理解的任务上。这包括构建更精巧的实验，避免因Token限制等非推理因素造成误判，以及更严格地评估模型在不确定和动态环境下的泛化能力。

从更深层次看，这场争论也促使业界重新审视AI的**认知架构**。如果LLM真的在复杂推理上存在根本性缺陷，那么单纯依靠扩大规模和数据量或许难以弥补。未来的突破点可能在于更深入地理解模型失效的根源，并探索**神经符号结合（Neuro-Symbolic AI）**等新架构。这种方法旨在整合深度学习的模式识别能力和符号AI的逻辑推理能力，使AI不仅能识别模式，更能进行可靠、可泛化的计算与推理，从而真正走向通用智能。

最终，这场争论不仅是关于技术能力的讨论，更是关于我们对“智能”本身理解的审视。它迫使我们从狂热的“AGI即将到来”的喧嚣中冷静下来，聚焦于人工智能发展中那些**棘手但至关重要**的根本性挑战。正如马库斯所言，如果连像Sam Altman这样的人都感到紧张，那是因为他们应该紧张。苹果的论文再次明确表明，规模化并非解决之道；这一次，人们终于开始关注这个问题了，而这恰恰是AI技术走向成熟和负责任发展的关键一步。

## 引用

[^1]: CRMArena-Pro：对不同业务场景和互动中的大语言模型智能体进行全面评估·arxiv.org·（2025/5/24）·检索日期2025/6/21
[^2]: 隐藏在显而易见的地方：视觉语言模型忽略了它们的视觉表现·arxiv.org·（2025/6/9）·检索日期2025/6/21
