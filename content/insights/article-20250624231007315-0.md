---
title: 边缘智能的突破：小米小爱同学如何在资源受限下实现高性能大模型推理
date: 2025-06-24T23:10:07+08:00
draft: false
featured_image: images/default (11).png
summary: 小米小爱同学团队在端侧大模型部署方面取得了显著进展，通过自研推理框架、动态优化、投机推理、量化以及创新的“共享基座+LoRA”架构，成功克服了移动设备资源限制，实现了高性能、多任务并发。文章深入剖析了小米的技术策略，并展望了未来硬件与模型架构（如Linear Attention）在推动端侧AI普惠化中的关键作用。
tags: 
  - 端侧AI
  - 大模型推理
  - 小米小爱同学
  - 资源受限
  - LoRA
  - 投机推理
  - 共享基座模型
  - Linear Attention
  - 边缘计算
  - AICon
main_topics: 
  - 算力与芯片
  - 前沿模型与算法
  - 产业生态与商业版图
---

> 在资源高度受限的移动和物联网设备上，小米小爱同学团队通过自研推理框架，实现了大模型每秒180个token的实时推理速度，并借助LoRA插件化与共享基座模型，有效解决了多业务并发与内存占用难题。这一系列工程创新不仅突破了端侧AI部署的技术瓶颈，更预示着通用人工智能普惠化的未来图景。

在人工智能的宏大叙事中，一个关键的分水岭正在出现：计算智能正从庞大的数据中心向我们手中的每一个设备延伸。将大型语言模型（LLMs）——这些拥有数十亿参数、能力惊人的数字大脑——部署到智能手机、汽车和物联网设备等边缘端，是实现AI无处不在的关键一步。然而，这并非易事。这些设备所面临的严苛资源限制，无论是计算能力、内存空间、功耗，还是模型更新机制的灵活性，都构成了巨大的工程挑战。小米小爱同学团队作为这一领域的先行者，正在通过一系列深思熟虑的技术策略，突破这些看似难以逾越的障碍。

### 突破边缘限制：小米的工程智慧

小米小爱同学端侧AI负责人杨永杰在近期的一次访谈中指出，尽管端侧大模型被广泛视为未来的重要方向，但其商业化落地进程相对缓慢，核心症结在于端侧设备固有的**资源限制**和大模型**快速迭代**的特性[^1]。云端大模型可以轻松承载数十甚至数百亿参数，且能快速更新。但端侧设备仅能支持参数量约40亿（4B）的模型，且即使经过低比特量化，也可能牺牲模型效果。此外，端侧模型更新机制滞后，难以跟上云端日新月异的迭代速度。杨永杰认为，目前的端侧大模型更像是“技术积累”，为未来硬件和模型成熟做准备[^1]。

面对这些挑战，小米团队选择了一条更为艰难但可能更具前景的道路：**自研大模型推理框架**。市场上针对端侧大模型的开源框架稀缺，尤其在NPU（神经网络处理器）上，由于芯片厂商通常不开放接口，导致NPU推理框架往往只能由厂商自行开发。这使得云端框架如vLLM或SGLang的丰富优化手段难以直接移植到端侧。小米的全栈自研，正是为了实现对每一个模块的细致性能优化，并能借鉴云端经验进行深度适配[^1]。

他们的努力取得了显著成果：实现了**180 tokens/s**的实时推理性能[^2]。这一速度在资源受限的端侧设备上堪称卓越，其背后凝结着多项系统级与模型级优化策略：

*   **动态输入与动态上下文支持**：传统NPU常采用静态图，要求固定输入尺寸，导致资源浪费。小米的框架通过自动切分输入尺寸，让模型在保持静态图性能的同时支持动态输入，大幅提升了资源利用率和吞吐率。
*   **投机推理（Speculative Decoding）优化**：这项技术在云端通常能带来2-3倍的加速，而小米通过自研策略在端侧实现了高达**7-10倍的解码加速**。这意味着原本可能只有20 tokens/s的速度，能够提升到足以媲美部分云端场景的200 tokens/s。
*   **量化与指令级优化**：通过对模型进行低比特量化，以及利用CPU的Neon指令集对量化、反量化、采样等关键操作进行加速，进一步榨取硬件性能。

这些工程实践不仅验证了端侧大模型的高性能潜力，也为车载等多模态应用场景的落地奠定了基础[^1]。

### 架构创新：多任务并发与资源复用

小爱同学作为一款语音助手，需要承载语音控制、多轮对话、智能家居等多种任务。虽然目前单个请求链路通常是串行的，但模型能力会被其他业务共用，导致业务间可能出现并发冲突。更深层次的挑战在于，端侧设备的NPU通常**不支持并发推理**，其硬件设计以串行执行为主。试图通过_multi-batch_提升并发效率在端侧也收效甚微，因为单条请求本身就可能已接近设备算力的上限[^1]。

为了在有限的存储空间和内存下支持多任务、多业务场景，小米团队采用了**“共享基座模型 + 插件化能力”**的创新架构。例如，一台12GB内存的手机部署一个4B大模型可能就需要3GB内存，实际可分配给大模型的空间可能更少。在这种背景下，为每个业务部署独立模型是不可行的。

小米的解决方案是：选择一个统一的基础大模型作为“基座”，然后针对不同的业务单独训练轻量级的**LoRA（Low-Rank Adaptation）模块**。当A业务的请求到来时，加载基座模型与A的LoRA进行推理；B业务请求到来时，则卸载A的LoRA，换上B的LoRA。这种机制在只保留一个基础模型的前提下，通过LoRA的插件化和轻量化特性，实现了在资源有限的设备上**动态切换不同业务能力**，从而支持多个任务的并发调用[^1]。这种“参数共享 + 差异定制”的模式，在内存利用率和扩展能力上都显示出显著优势。

在跨芯片平台部署方面，小米的推理框架采用了模块化、后端解耦的设计，与传统模型框架处理多后端问题异曲同工。杨永杰指出，大模型虽然规模庞大，但在框架设计层面，与底层硬件的绑定程度反而没有传统模型那么深，使得抽象出通用接口、实现跨平台迁移变得相对容易[^1]。

性能优化策略的组合与优先级也是工程实践中的关键。小米团队倾向于优先实现那些技术价值大、适用面广、且与其他手段不冲突的优化方式，例如**并行解码、低比特量化、带宽控制**等都可以组合使用。对于并非所有业务都需要的优化（如_prompt cache_），则将其封装为业务可选的配置项，降低使用复杂度，同时提供针对性的性能提升建议[^1]。

### 展望未来：算力与模型架构的双轮驱动

在杨永杰看来，端侧大模型未来最具突破性的方向将集中在两大核心目标：

1.  **面向大模型优化的硬件能力提升**：当前端侧大模型的诸多限制，如功耗高、挤占其他业务资源导致系统卡顿，都直接源于硬件算力不足。如同当年传统AI模型兴起催生了NPU，杨永杰预计，随着大模型热度持续，新一代面向大模型的端侧芯片将应运而生。一旦这类硬件普及，端侧模型的能力将得到大幅增强，从而让更多业务真正落地，实现商业化[^1]。
2.  **模型架构的演进**：目前主流大模型多基于Transformer架构，其自回归特性导致当上下文（context）变长时，资源占用（特别是KV cache）会显著增加，内存和算力压力陡增。业界正在探索的**Linear Attention**架构，如**Mamba、RWKV**等，尝试在保持模型能力的同时，使内存使用与输入长度无关。这项技术对于资源敏感的端侧场景至关重要，尤其是在多模态任务中，图片、视频等非文本输入会迅速拉长上下文，Transformer的瓶颈将愈发凸显。尽管Linear Attention目前整体效果尚不及Transformer，但其研究热度与潜力预示着未来实用级别突破的可能性[^1]。

小米小爱同学团队的实践，无疑为全球范围内的边缘AI部署提供了宝贵的经验和范例。这不仅仅是技术性能的简单提升，更是对AI普惠化未来的一次深度探索。当AI不再局限于云端，而是真正融入我们日常使用的每一个设备时，它将以更低的延迟、更高的隐私保护、更强大的离线能力，深刻改变我们与数字世界的互动方式。

## 引文
[^1]: <a href="https://www.infoq.cn/article/ifffkybhkhafkxdi4iac" target="_blank">小爱同学在高性能端侧大模型推理的实践｜AICon北京</a>·InfoQ·罗燕珊（2025/6/24）·检索日期2025/6/24
[^2]: <a href="https://www.163.com/dy/article/K2R51P7D0511D3QS.html" target="_blank">小米小爱同学：资源受限下，实现端侧大模型的高性能推理</a>·网易（2025/6/24）·检索日期2025/6/24
[^3]: <a href="http://www.itbear.com.cn/html/2025-06/863485.html" target="_blank">小米小爱同学如何突破资源限制，实现端侧大模型高效推理？</a>·ITbear（2025/6/24）·检索日期2025/6/24
[^4]: <a href="https://www.showapi.com/news/article/684122664ddd79013c00132f" target="_blank">端侧大模型推理：小爱同学AICon会议的实践之路</a>·万维易源（2025/6/24）·检索日期2025/6/24
