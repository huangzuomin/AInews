---
title: 大语言模型如何被一场古老棋局“考倒”：ChatGPT与“理解”的边界
date: 2025-06-17T20:20:00+08:00
draft: false
featured_image: images/default (12).png
summary: 一场ChatGPT与1979年《Video Chess》的对局以大语言模型惨败告终，暴露了其在处理离散、规则严格的状态追踪任务上的固有弱点。此次事件引发了对当前AI能力，尤其是LLM“理解”边界的深刻反思，提醒业界和公众需更清醒地认识到AI的局限性，并呼吁构建更符合任务需求的混合AI系统。
tags: 
  - 大语言模型
  - ChatGPT
  - 人工智能局限
  - 象棋AI
  - 技术盲点
  - 状态追踪
  - AI伦理
  - 技术审视
main_topics: 
  - AI能力边界
  - 大语言模型原理
  - 技术与社会影响
---

> 一场ChatGPT与1979年古老象棋程序《Video Chess》的对决，以大语言模型的惨败告终，这场看似滑稽的“翻车”揭示了当前AI在离散状态追踪和逻辑推理上的固有局限。它迫使我们重新审视，在生成式AI热潮下，我们对“人工智能”的定义与期待是否被过度泛化。

当今AI领域，大语言模型（LLM）无疑是明星，其在文本生成、对话、翻译等方面的表现令人惊叹。然而，最近一场ChatGPT与一台诞生于1979年、频率仅1.19 MHz的8位主机Atari 2600上的《Video Chess》象棋游戏的对决，却像一盆冷水，浇醒了部分对AI能力盲目乐观的认知。在这场看似实力悬殊的较量中，ChatGPT不仅未能如其所愿“很快就能赢”，反而频频“翻车”，暴露了其在特定任务上的技术盲点和“理解”的边界。

### 大语言模型的固有盲点：状态追踪的困境

这场由基础架构工程师Robert Jr. Caruso发起的实验，最初源于他与ChatGPT关于象棋AI的对话。当ChatGPT主动提出挑战这款简陋的《Video Chess》时，自信满满地认为其将轻而易举地获胜。然而，90分钟的对局却演变成了一场令人啼笑皆非的“闹剧”[^1]。ChatGPT不仅屡次**认错棋子**，将车当象、错失关键战机，甚至连哪些棋子已被吃掉都无法记住。它会建议“牺牲骑士去换兵”这种荒谬的策略，并不断要求“从头再来”，仿佛重置棋局就能摆脱其内在的混乱。Caruso不得不全程人工干预，帮助模型“回忆”棋盘局面，最终ChatGPT主动认输。

这并非偶然的失误，而是大语言模型底层架构所决定的固有局限。如多位观察者所指出的，GPT这类模型的核心机制是通过分析海量文本数据，学习词语、句子、段落之间的概率关联性，并基于此进行预测和生成。它们擅长的是_“捕捉序列概率”_和_“填充空白”_，在语言和文本的连续性、开放性任务中表现卓越[^2]。

然而，象棋这类离散系统，其本质是高度结构化、规则严格且对**状态追踪**要求极高的任务。64个格子、32个棋子，每一步棋都对棋盘的整体状态产生精确、不可逆转的影响，且没有容错空间。传统的象棋AI，如Stockfish或AlphaZero，其内部都有明确定义的棋盘数据结构、规则引擎和深度搜索算法。它们能精准地表达和更新棋盘状态，并通过计算和搜索得出最佳落子。

ChatGPT则不然。它每一次的“发言”都是基于当前的“语言提示”和其训练数据中习得的模式，它没有一个**“内部记忆”或“状态表达结构”**来持续、准确地追踪棋盘上所有棋子的位置和状态。尽管可以通过外部工程手段，如在提示词中不断输入当前的棋盘状态，来模拟记忆，但模型在多轮对话后仍会“混淆上下文”[^1]。这就像网友评论的那样：“你拿一个聊天机器人去玩象棋，就像用千斤顶修车。”[^1] LLM在高度组合性的任务面前，其基于统计关联的范式显得力不从心。

### 超越棋盘：对AI“理解”与“能力”边界的再审视

这场惨败远不止于一个AI“智力”测试的失败，它深刻地触及了当前关于大语言模型“理解”能力的本质问题。如果一个模型无法在清晰、确定规则的象棋游戏中维持其内部逻辑状态，那么在更复杂、更模糊的现实场景中，我们又该如何评估其逻辑一致性和可靠性？

例如，在**自动驾驶**的决策路径规划中，系统需要精确地追踪路况、车辆、行人等多个动态变量的状态，并在此基础上做出毫秒级的安全决策。在**金融模型的风险评估**中，任何对市场状态或资产状况的混淆都可能导致灾难性的后果。甚至在看似擅长的**多轮对话**中，如果LLM无法在深层次上“记住”用户的情绪变化或先前的约定，其“理解”和“共情”也将流于表面。

这引发了一个更深层次的疑问：我们所理解的“AI能力”，是否在当前的宣传和炒作中被过度包装和泛化了？大语言模型无疑展现了惊人的“涌现能力”（_Emergence Phenomenon_）[^2]，即模型规模达到一定程度后，会表现出超越预期的复杂行为和能力。但这种涌现并非意味着模型获得了真正意义上的“理解”或“通用智能”。它更像是通过海量数据压缩和模式识别，实现了对人类知识和推理过程的**模拟和近似**，而非内化了其核心的逻辑和因果关系。

此次事件提醒我们，AI并非万能，不同的任务需要不同范式的AI。大语言模型作为强大的语言处理工具，其价值毋庸置疑，但它并非解决所有问题的“万能药”。对于需要精确状态追踪、严格逻辑推理和因果理解的任务，我们可能需要结合符号AI、强化学习或其他专门设计的架构，构建**混合AI系统**，而非期望单一的LLM能包揽一切。

这促使我们以更为审慎和批判的眼光来看待当前AI技术的发展。在AI技术以前所未有的速度渗透社会各个角落之际，清晰地认识到它们的优势与局限，对于负责任地开发、部署和应用人工智能至关重要。只有这样，我们才能避免在未来的科技洪流中，因盲目的乐观而付出不必要的代价。

## References

[^1]: 大数据文摘（2025/6/17）。LLM 翻车现场，ChatGPT 挑战 1979《Video Chess》惨败：连车马象都认错。51CTO.COM。检索日期2025/6/17。
[^2]: 血衫非弧の一存（2023/7/21）。大语言模型进化之谜：涌现现象的挑战与争议 与 由ChatGPT反思大语言模型（LLM）的技术精要。blog.kelu.org。检索日期2025/6/17。
[^3]: 知乎（不详）。让「ChatGPT」们爆火的 LLM 模型到底是什么？。知乎。检索日期2025/6/17。
[^4]: hitripod.com（不详）。ChatGPT 與 LLM 的技術原理剖析、發展歷程。hitripod.com。检索日期2025/6/17。
