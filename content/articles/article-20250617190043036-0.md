---
title: “思考的幻象”还是评估的盲点？AI推理能力辩论的深层反思
date: 2025-06-17T19:00:43+08:00
draft: false
summary: 苹果公司发布论文《思考的幻象》，质疑大语言模型（LLM）的推理能力，认为其在复杂逻辑任务中性能崩盘。然而，一篇由独立研究员Alex Lawsen和Claude Opus 4共同撰写的反驳论文《思考的幻象的幻象》指出，苹果的评估存在严重缺陷，包括对Token输出限制的忽视、测试题目不严谨以及评估方式的片面性。这场辩论不仅揭示了当前AI评估方法的局限性，也引发了对AI智能本质和未来发展路径的深层思考。
tags: 
  - 人工智能
  - 大语言模型
  - 推理能力
  - 评估方法
  - 伦理影响
  - Claude
  - Apple
  - AI哲学
  - 技术争议
main_topics: 
  - AI推理能力争议
  - 大模型评估方法论
  - 人工智能伦理
---

> 苹果公司的一篇论文质疑大语言模型（LLM）的推理能力，称其“思考是幻象”。然而，一篇由Claude Opus 4共同撰写的反驳论文指出，苹果的实验设计存在严重缺陷，实际问题并非模型缺乏推理能力，而是受限于输出长度、测试题目失当以及评估方式的片面性，引发了对AI能力边界和测试方法论的深刻辩论。

近期，人工智能领域再次被一场激烈的学术争论搅动，核心焦点直指大语言模型（LLM）最引以为傲的能力之一：推理。这场风波的起点是科技巨头苹果公司机器学习研究团队发布的一篇名为《思考的幻觉（The Illusion of Thinking）》的论文[^1]。这份长达53页的技术报告，以一系列经典逻辑谜题——**汉诺塔（Tower of Hanoi）、积木世界（Blocks World）、过河问题（River Crossing）和跳棋（Checkers Jumping）**——对当前主流LLM的推理能力提出了尖锐质疑。

苹果的研究团队通过这些随着步骤和限制条件增加而难度指数级飙升的任务，发现包括OpenAI的“o”系列、Google的Gemini 2.5以及DeepSeek-R等“推理型大模型”，在面对复杂任务时，其准确率会直线下滑，最终在最复杂场景下甚至**性能崩盘，准确率归零**。更引人注意的是，模型用于“思考”的篇幅（即输出的token数量）也随之缩水，这被苹果研究者解读为模型主动减少推理尝试的迹象，从而得出结论：大型语言模型的推理能力，可能只是一种“幻象”，并未真正学习到可泛化的第一性原理，而更像是记忆模式的复读机。

### 推理的幻象：苹果的质疑与评估陷阱

苹果的论文一经发布，便在学术界和社交媒体上掀起轩然大波，许多人据此宣称，主流AI的“推理能力”已被证伪。然而，争议的火苗很快被一篇名为《思考的幻象的幻象（The Illusion of The Illusion of Thinking）》的反驳论文点燃[^2]。令人瞩目的是，这篇论文的作者是独立研究员Alex Lawsen，以及他的“合作伙伴”——大语言模型**Claude Opus 4**。这本身就构成了一个强烈的声明：一个被质疑推理能力的AI，亲自参与撰写了一篇反驳其缺乏推理能力的论文。

反驳论文的核心论点是：苹果所谓的“推理崩溃”，并非AI能力的上限，而是其**实验设计本身存在致命缺陷**。

首先，也是最核心的反驳点，在于**混淆了“推理失败”和“作文本不够长”**。像汉诺塔这类问题，解决步骤是随着盘子数量指数级增长的。例如，一个15个盘子的汉诺塔问题，理论上需要输出超过32,000个步骤。然而，现有大模型的上下文窗口和单次输出Token都有明确的上限。批评者指出，模型很可能在内部已经推导出了正确的算法和策略，但因为输出篇幅的限制，导致答案被截断，从而被苹果的评估脚本直接判了零分。这并非是逻辑能力的极限，而是_Token输出能力的极限_。正如反驳方所言：“这不是逻辑的极限，这是Token的极限。”[^3]

其次，反驳论文进一步指出，苹果用来测试的**“考卷”本身就存在设计缺陷**。在苹果使用的某些“过河问题”的基准测试中，根据其给出的限制条件，一些题目在数学上是**根本无解的**。AI自然无法给出“正确答案”。但更令人费解的是，苹果的评估系统却依然对模型在这些无解题上的输出进行了评分，并以此作为模型“失败”的证据。这种对测试题目的严谨性审查不足，无疑削弱了其结论的说服力。

### 从Token到算法：重新审视AI的“思考”方式

最有力且最具说服力的反驳实验，是“换个‘考法’，AI原地复活”。Alex Lawsen和Claude Opus 4做了一个简单的实验：他们不再要求模型一步一步地写出汉诺塔的完整解法（这种“默写全文”式的要求），而是让模型**直接输出一个能解决这个问题的“程序代码”（例如一个Lua函数）**。

结果令人惊讶：模型在之前被判定为“彻底失败”的、更复杂的任务上，轻松给出了正确的程序。这个实验有力地证明了，AI并非“不懂”解题的逻辑，它只是无法遵循那种极其冗长且低效的“口述”式输出要求。AI的“大脑”里可能已经形成了算法，但我们却强求它将每一步计算都以人类习惯的方式“口述”出来。这引发了一个深刻的思考：我们对AI“思考”方式的理解，是否过于人类中心化？

此外，有其他研究者在社交媒体X（原Twitter）上指出了苹果实验的第四个槽点：**缺乏人类基准的“单方面宣布”**。苹果在整个实验中，从未将模型的表现与人类在相同任务下的表现进行对比。一个正常人在没有任何纸笔辅助的情况下，去心算一个需要几百步规划的逻辑谜题，其大脑也同样会“宕机”。没有这个最基本的参照系，我们又怎能断言AI的“性能衰减”是一种根本性的“思考缺陷”，而非所有智能体（包括人类）在面对超限复杂任务时的正常表现呢？

### 智能边界的辩论：技术与伦理的交织

这场关于“思考的幻象”的辩论，远超出了单纯的技术细节讨论，它触及了我们对AI智能本质的理解，以及如何公正、有效地评估这些日益复杂系统的核心问题。它揭示了当前AI评估体系可能存在的盲点和局限性。

首先，它迫使我们重新思考**何谓“推理”**。如果AI能够生成解决问题的代码，这是否足以证明其理解了底层逻辑？人类的推理往往伴随着语言和思维链条的表达，但AI的“思维”可能以更隐蔽、更高效的非语言形式存在。将人类的认知模式强加于AI，可能限制了我们对其真实能力的认知。

其次，这场辩论凸显了**评估方法论的关键性**。一个不严谨、不全面的测试，可能得出误导性的结论，从而影响公众对AI的认知，甚至左右未来的研发方向和投资。我们需要设计出更具鲁棒性、能适应AI独特处理方式的基准测试，而不是简单地将人类的评估标准“硬套”在AI身上。这不仅关乎技术进步，更关乎科学严谨性与伦理责任。

最后，Claude Opus 4作为合著者参与反驳论文，本身就是一件具有里程碑意义的事件，预示着AI在未来的学术和技术讨论中，将扮演更加积极、甚至“独立”的角色。这不仅挑战了传统的科学研究范式，也引发了关于**AI主体性、责任归属以及知识产权**等一系列深层伦理问题。

“思考的幻象”与“幻象的幻象”之间的较量，并非简单的技术分歧，而是对AI未来发展路径的一次深刻反思。它提醒我们，在惊叹于AI进步的同时，更要审慎地理解其能力边界，不断完善评估工具，并以开放的心态探索智能的多样形态。只有这样，我们才能真正走向人机协作的智能未来。

## References
[^1]: Apple Machine Learning Research (2025/6/17). [The Illusion of Thinking](https://www.apple.com/research/docs/The_Illusion_of_Thinking.pdf). Apple. Retrieved 2025/6/17.
[^2]: Lawsen, Alex & Claude Opus 4 (2025/6/17). [The Illusion of The Illusion of Thinking](https://mp.weixin.qq.com/s/hlg87MrNa0mLnCCoY5oChQ). 大数据文摘. Retrieved 2025/6/17.
[^3]: 大数据文摘 (2025/6/17). [反转，AI推理能力遭苹果质疑后，Claude合著论文反击：不是不会推理，是输给Token](https://news.qq.com/rain/a/20250617A05MCC00). 腾讯新闻. Retrieved 2025/6/17.
